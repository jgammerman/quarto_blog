---
title: "Text Classification with LLMs (part 1)"
description: "Latest"
author: "James Gammerman"
date: "10/29/2024"
draft: false
format:
  html:
    code-fold: false
jupyter: thellmbook
execute:
    cache: true
    kernel: thellmbook
    # python: /Users/yasha/miniconda3/envs/thellmbook/bin/python
bibliography: references.bib
# crossref: true
---

# Introduction

Welcome to my first blog post!

I'm currently reading the excellent [Hands-On Large Language Models](https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/) book by Jay Alammar and Maarten Grootendorst. I highly recommend it to anyone looking to up their LLM skills!

In order to get the most out of these kinds of books, I find it's best to take the code in them and then adapt it to a new dataset. So in that spirit, I'm going to do my own version of Chapter 4, which is all about *text classification*.

There are many different kinds of LLM. Broadly speaking we can put them into two categories:

1.  **Representation models** are designed to understand and represent the meaning of text. They convert input into dense embeddings that capture its semantic information. These embeddings can either be used directly for classification or as input to a traditional classifier, depending on the task. These models typically use an encoder-only architecture, and notable examples include BERT, RoBERTa, and Sentence-BERT.

2.  **Generative models**, on the other hand, focus on generating new text. They are trained to predict the next word in a sequence, allowing them to produce text that resembles the training data. For classification, they can be adapted by using a carefully crafted prompt that guides them to generate a response corresponding to the class label. Generative models use either decoder-only or encoder-decoder architectures. Examples include the GPT family (including ChatGPT), Flan-T5, and even DALL-E for image generation.

The difference between the two is shown in @fig-rep-vs-gen-models. In this blog post we'll focus on representation models, and in part 2 we'll look at generative models.

![Both representation and generative models can be used for classification, but they take different approaches. Taken from the book.](images/rep-models-vs-gen-models.jpg){#fig-rep-vs-gen-models}

## Text classification with representation models

When used for classification tasks, representation models can be broadly divided into:

1.  **Task-Specific Models**: These models involve starting with a general-purpose pre-trained model (such as BERT) and fine-tuning it directly for a specific task, such as sentiment classification. They take the input text and output the class label directly, making it a one-step process that is optimised for classification.

2.  **Embedding Models**: These models are fine-tuned to generate embeddings that capture the meaning of the text. The embeddings are then used as input features for a separate, traditional classifier (e.g., logistic regression) to predict the class label. So this can be thought of as a two-step process: first, encode the text, and secondly classify it with a simpler model.

@fig-2-types-rep-models illustrates these two approaches.

![Classification can be done directly with a task-specific model or indirectly with an embedding model](images/2-types-of-rep-models.jpg){#fig-2-types-rep-models}

# The data

The dataset we will use is the **Amazon Polarity Dataset**. This dataset contains reviews from Amazon, categorised as either positive or negative sentiment. Each entry consists of a title, the review text, and the associated sentiment label.

We start by loading the data and splitting into a training and test set (no need for a validation set here as we won't be tuning any hyperparameters):

```{python}
#| label: Load data
#| cache: true
#| output: false
from datasets import load_dataset

# Load our data
data = load_dataset("amazon_polarity")

# Take a random sample of 10k training examples and 2k test examples
train_sample = data["train"].shuffle(seed=42).select(range(10000))
test_sample = data["test"].shuffle(seed=42).select(range(2000))
```

Let's check how many positive and negative labels we have in each set:

```{python}
#| cache: true

from collections import Counter

# Count the number of occurrences of each label in the training and test data
label_counts = Counter(train_sample["label"])
print(f"Label Counts in Training Set: {label_counts}")
print()
label_counts = Counter(test_sample["label"])
print(f"Label Counts in Test Set: {label_counts}")

```

So we have a balanced dataset. A label of `1` indicates a positive review, and `0` a negative review.

Let's take a quick look at a couple of examples to understand the structure of the dataset:

```{python}
#| cache: true

import json
print(json.dumps(train_sample[0], indent=4))
print(json.dumps(train_sample[2], indent=4))
```

So each review is a dictionary containing a `label`, `title` and `content` key.

# 1. Task-specific model

## Selecting the model

So which LLM should we use? As of now, there are over 60,000 models available on Hugging Face for text classification and 8,000 for embeddings. As ever with LLMs, picking the right one involves a trade-off between size and performance.

As mentioned earlier, BERT is a popular architecture for creating both task-specific and embedding models. Many variations of it are now available. In the book they used one called`RoBERTa`, but here we will use a variation of `DistilBERT` called `DistilBERT base model (uncased)`, a lighter and faster version of BERT that is fine-tuned specifically for sentiment analysis. So it should give good results!

## Loading the model

Now that we've picked our task-specific representation model, we can proceed by loading the model:

```{python}
#| eval: true
#| cache: true
#| label: Load model

# Import the pipeline function from the transformers library
from transformers import pipeline

# Path to HF model
model_path = "distilbert-base-uncased-finetuned-sst-2-english"

# Load the model into a pipeline
pipe = pipeline(
    model=model_path,
    tokenizer=model_path,
    # return_all_scores=True,
    top_k=None,
    # device="-1"
    # device="cuda:0" # change this to "-1" if you only have access to a CPU not GPU
)
```

## Testing the model out

Let's use our pre-trained BERT-based model to classify some sample reviews from our dataset. We'll run it on an example to see how well it predicts the sentiment:

```{python}
#| eval: false
#| cache: true
sample_review = train_sample[8]["content"]

result = pipe(sample_review)
print(f"Review: {sample_review}")
print(f"Sentiment Analysis Result: {result}")
```

```{python}
#| eval: false
#| cache: true
sample_review = train_sample[8]["content"]

result = pipe(sample_review)

# Split the review into lines with a maximum width
import textwrap
wrapped_review = textwrap.fill(sample_review, width=80) 

print(f"Review:\n{wrapped_review}")

# Format the sentiment analysis result with indentation
import json
print(f"Sentiment Analysis Result:\n{json.dumps(result, indent=2)}")
```

Looks good! Now we can run it on the full test set.

## Evaluation

```{python}
#| eval: false
#| cache: true
import numpy as np
from tqdm import tqdm
from transformers.pipelines.pt_utils import KeyDataset  # for efficient data loading

# Empty list to store predictions
y_pred = []

# Iterate through the test data and make predictions
for output in tqdm(pipe(KeyDataset(test_sample, "content"), batch_size=8), total=len(test_sample)):
    # Extract negative and positive sentiment scores from the pipeline's output
    negative_score = output[0]["score"]
    positive_score = output[1]["score"]
    assignment = np.argmax([negative_score, positive_score])  # Determines the predicted class
    y_pred.append(assignment)  

# Display the first 10 predictions
print(f"First 10 Predictions: {y_pred[:10]}")

```

So we've got our predictions for the test set. Let's create a classification report which we can use for evaluation:

```{python}
#| eval: false
#| cache: true
from sklearn.metrics import classification_report

# Extract true labels for the sampled data
y_true = test_sample["label"]

# Define a function to evaluate performance
def evaluate_performance(y_true, y_pred):
    """Create and print the classification report"""
    performance = classification_report(
        y_true, y_pred,
        target_names=["Negative Review", "Positive Review"]
    )
    print(performance)

# Evaluate the model performance
evaluate_performance(y_true, y_pred)
```

![test caption](images/classification_rep.jpg){}

Looks like our model did a good job! An accuracy of 88\% is pretty impressive given that our pretrained BERT model wasn't trained specifically on our Amazon reviews data ðŸ˜Š

Note that the labels in this data are well-balanced, so accuracy is a good metric to use. If they were imbalanced we might focus more on precision/recall/f1-score, which also have high values in this instance.

So we've tried out a task-specific representation model. Now we can test the other kind of representation model: embedding models.

# 2. Embedding model

In the last section, we used a pre-trained task-specific model for classifying sentiment. But what if we don't have a model to hand which was pre-trained for our specific task?

Fortunately we don't need to fine-tune a model ourselves, which can be very expensive. Instead of using a representation model directly, in this section we will use an embedding model to create features from our text. These features can then be used by a separate classifier to predict sentiment. So in a sense, we'll be doing a part of the training ourselves.

We'll do it in two steps. Firstly, we'll change our foundation model from being based on the BERT architecture to one based on the MPNet architecture, which is an improved version of BERT addressing some of its limitations. The specific model we'll use is called `all-mpnet-base-v2`, and to access it we'll use the Sentence Transformers library:

```{python}
#| eval: false
#| cache: true
from sentence_transformers import SentenceTransformer

# Load model - all-mpnet-base-v2
embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')

# Alternative - load a BERT-based model
# model_name = 'bert-base-uncased'
# embedding_model = SentenceTransformer(model_name)

# Convert text to embeddings
train_embeddings = embedding_model.encode(train_sample["content"], show_progress_bar=True)
test_embeddings = embedding_model.encode(test_sample["content"], show_progress_bar=True)
```

Let's look at the dimensions of our new embedding:

```{python}
#| eval: false
#| cache: true
train_embeddings.shape
```

![test caption](images/embedding_shape.jpg){}

Blah blah

# References