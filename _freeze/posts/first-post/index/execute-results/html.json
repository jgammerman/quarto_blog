{
  "hash": "12556f2c94c0dc8ab4a0021878b1ffe4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Text Classification with LLMs (part 1)\"\ndescription: \"TODO\"\nauthor: \"James Gammerman\"\ndate: \"10/29/2024\"\ndraft: false\nformat:\n  html:\n    code-fold: false\njupyter: thellmbook\nexecute:\n    cache: true\n    kernel: thellmbook\n    # python: /Users/yasha/miniconda3/envs/thellmbook/bin/python\nbibliography: references.bib\n# crossref: true\n---\n\n\n\n\n# Introduction\n\nWelcome to my first blog post!\n\nI'm currently reading the excellent [Hands-On Large Language Models](https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/) book by Jay Alammar and Maarten Grootendorst. I highly recommend it to anyone looking to up their LLM skills!\n\nIn order to get the most out of these kinds of books, I find it's best to take the code in them and then adapt it to a new dataset. So in that spirit, I'm going to do my own version of Chapter 4, which is all about *text classification*.\n\nThere are many different kinds of LLM. Broadly speaking we can put them into two categories:\n\n1.  **Representation models** are designed to understand and represent the meaning of text. They convert input into dense embeddings that capture its semantic information. These embeddings can either be used directly for classification or as input to a traditional classifier, depending on the task. These models typically use an encoder-only architecture, and notable examples include BERT, RoBERTa, and Sentence-BERT.\n\n2.  **Generative models**, on the other hand, focus on generating new text. They are trained to predict the next word in a sequence, allowing them to produce text that resembles the training data. For classification, they can be adapted by using a carefully crafted prompt that guides them to generate a response corresponding to the class label. Generative models use either decoder-only or encoder-decoder architectures. Examples include the GPT family (including ChatGPT), Flan-T5, and even DALL-E for image generation.\n\nThe difference between the two is shown in @fig-rep-vs-gen-models. In this blog post we'll focus on representation models, and in part 2 we'll look at generative models.\n\n![Both representation and generative models can be used for classification, but they take different approaches. Taken from the book.](images/rep-models-vs-gen-models.jpg){#fig-rep-vs-gen-models}\n\n## Text classification with representation models\n\nWhen used for classification tasks, representation models can be broadly divided into:\n\n1.  **Task-Specific Models**: These models involve starting with a general-purpose pre-trained model (such as BERT) and fine-tuning it directly for a specific task, such as sentiment classification. They take the input text and output the class label directly, making it a one-step process that is optimised for classification.\n\n2.  **Embedding Models**: These models are fine-tuned to generate embeddings that capture the meaning of the text. The embeddings are then used as input features for a separate, traditional classifier (e.g., logistic regression) to predict the class label. So this can be thought of as a two-step process: first, encode the text, and secondly classify it with a simpler model.\n\n@fig-2-types-rep-models illustrates these two approaches.\n\n![Classification can be done directly with a task-specific model or indirectly with an embedding model](images/2-types-of-rep-models.jpg){#fig-2-types-rep-models}\n\n# The data\n\nThe dataset we will use is the **Amazon Polarity Dataset**. This dataset contains reviews from Amazon, categorised as either positive or negative sentiment. Each entry consists of a title, the review text, and the associated sentiment label.\n\nWe start by loading the data and splitting into a training and test set (no need for a validation set here as we won't be tuning any hyperparameters):\n\n::: {#load-data .cell cache='true' execution_count=2}\n``` {.python .cell-code}\nfrom datasets import load_dataset\n\n# Load our data\ndata = load_dataset(\"amazon_polarity\")\n\n# Take a random sample of 10k training examples and 2k test examples\ntrain_sample = data[\"train\"].shuffle(seed=42).select(range(10000))\ntest_sample = data[\"test\"].shuffle(seed=42).select(range(2000))\n```\n:::\n\n\nLet's check how many positive and negative labels we have in each set:\n\n::: {#ff5f18fc .cell cache='true' execution_count=3}\n``` {.python .cell-code}\nfrom collections import Counter\n\n# Count the number of occurrences of each label in the training and test data\nlabel_counts = Counter(train_sample[\"label\"])\nprint(f\"Label Counts in Training Set: {label_counts}\")\nlabel_counts = Counter(test_sample[\"label\"])\nprint(f\"Label Counts in Test Set: {label_counts}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLabel Counts in Training Set: Counter({0: 5003, 1: 4997})\nLabel Counts in Test Set: Counter({1: 1018, 0: 982})\n```\n:::\n:::\n\n\nSo we have a balanced dataset. A label of `1` indicates a positive review, and `0` a negative review.\n\nLet's take a quick look at a couple of examples to understand the structure of the dataset:\n\n::: {#1a223f6d .cell cache='true' execution_count=4}\n``` {.python .cell-code}\n# import json\n# print(json.dumps(train_sample[0], indent=4))\n# print(json.dumps(train_sample[5], indent=4))\n\nprint(train_sample[0])\nprint(train_sample[2])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'label': 0, 'title': 'Anyone who likes this better than the Pekinpah is a moron.', 'content': \"All the pretty people in this film. Even the Rudy character played by Michael Madsen. This is adapted from a Jim Thompson novel for cryin' out loud! These are supposed to be marginal characters, not fashion models. Though McQueen and McGraw were attractive (but check out McQueen's crummy prison haircut) they were believable in the role. Baldwin and Bassinger seem like movie stars trying to act like hard cases. Action wise, the robbery scene in the Pekinpah version was about 100 times more exciting and suspenseful than anything in this re-make.\"}\n{'label': 1, 'title': 'Spaetzle Noodles', 'content': \"This type of spaetzle maker is easier to manuveur than the old press kind and much easier on the hands. The difference is that this new spaetzle maker makes smaller noodles than the old. It is great for us elderly that don't have much strength left.\"}\n```\n:::\n:::\n\n\nSo each review is a dictionary containing a `label`, `title` and `content` key.\n\n# 1. Task-specific model\n\n## Selecting the model\n\nSo which LLM should we use? As of now, there are over 60,000 models available on Hugging Face for text classification and 8,000 for embeddings. As ever with LLMs, picking the right one involves a trade-off between size and performance.\n\nAs mentioned earlier, BERT is a popular architecture for creating both task-specific and embedding models. Many variations of it are now available. In the book they used one called`RoBERTa`, but here we will use a variation of `DistilBERT` called `DistilBERT base model (uncased)`, a lighter and faster version of BERT that is fine-tuned specifically for sentiment analysis. So it should give good results!\n\n## Loading the model\n\nNow that we've picked our task-specific representation model, we can proceed by loading the model:\n\n::: {#load-model .cell cache='true' execution_count=5}\n``` {.python .cell-code}\n# Import the pipeline function from the transformers library\nfrom transformers import pipeline\n\n# Path to HF model\nmodel_path = \"distilbert-base-uncased-finetuned-sst-2-english\"\n\n# Load the model into a pipeline\npipe = pipeline(\n    model=model_path,\n    tokenizer=model_path,\n    # return_all_scores=True,\n    top_k=None,\n    device=\"cuda:0\" # change this to \"-1\" if you only have access to a CPU not GPU\n)\n```\n:::\n\n\n## Testing the model out\n\nLet's use our pre-trained BERT-based model to classify some sample reviews from our dataset. We'll run it on a couple to see how well it predicts the sentiment:\n\n::: {#21b2ebea .cell cache='true' execution_count=6}\n``` {.python .cell-code}\nsample_review = train_sample[8][\"content\"]\n\nresult = pipe(sample_review)\nprint(f\"Review: {sample_review}\")\nprint(f\"Sentiment Analysis Result: {result}\")\n```\n:::\n\n\n![](images/1.jpg)\n\nBlah blah\n\n# References\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}