[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "James Gammerman’s Blog",
    "section": "",
    "text": "Sample Document\n\n\n\n\n\n\n\n\n\n\n\nYour Name\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 29, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nText Classification with LLMs (part 1)\n\n\n\n\n\nTODO\n\n\n\n\n\nOct 29, 2024\n\n\nJames Gammerman\n\n\n\n\n\n\n\n\n\n\n\n\nText Classification with LLMs (part 1)\n\n\n\n\n\nTODO\n\n\n\n\n\nOct 29, 2024\n\n\nJames Gammerman\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/first-post/index.html",
    "href": "posts/first-post/index.html",
    "title": "Text Classification with LLMs (part 1)",
    "section": "",
    "text": "Welcome to my first blog post!\nI’m currently reading the excellent Hands-On Large Language Models book by Jay Alammar and Maarten Grootendorst. I highly recommend it to anyone looking to up their LLM skills!\nIn order to get the most out of these kinds of books, I find it’s best to take the code in them and then adapt it to a new dataset. So in that spirit, I’m going to do my own version of Chapter 4, which is all about text classification.\nThere are many different kinds of LLM. Broadly speaking we can put them into two categories:\n\nRepresentation models are designed to understand and represent the meaning of text. They convert input into dense embeddings that capture its semantic information. These embeddings can either be used directly for classification or as input to a traditional classifier, depending on the task. These models typically use an encoder-only architecture, and notable examples include BERT, RoBERTa, and Sentence-BERT.\nGenerative models, on the other hand, focus on generating new text. They are trained to predict the next word in a sequence, allowing them to produce text that resembles the training data. For classification, they can be adapted by using a carefully crafted prompt that guides them to generate a response corresponding to the class label. Generative models use either decoder-only or encoder-decoder architectures. Examples include the GPT family (including ChatGPT), Flan-T5, and even DALL-E for image generation.\n\nThe difference between the two is shown in Figure 1. In this blog post we’ll focus on representation models, and in part 2 we’ll look at generative models.\n\n\n\n\n\n\nFigure 1: Both representation and generative models can be used for classification, but they take different approaches. Taken from the book.\n\n\n\n\n\nWhen used for classification tasks, representation models can be broadly divided into:\n\nTask-Specific Models: These models involve starting with a general-purpose pre-trained model (such as BERT) and fine-tuning it directly for a specific task, such as sentiment classification. They take the input text and output the class label directly, making it a one-step process that is optimised for classification.\nEmbedding Models: These models are fine-tuned to generate embeddings that capture the meaning of the text. The embeddings are then used as input features for a separate, traditional classifier (e.g., logistic regression) to predict the class label. So this can be thought of as a two-step process: first, encode the text, and secondly classify it with a simpler model.\n\nFigure 2 illustrates these two approaches.\n\n\n\n\n\n\nFigure 2: Classification can be done directly with a task-specific model or indirectly with an embedding model"
  },
  {
    "objectID": "posts/first-post/Blog_post_on_text_classification.html",
    "href": "posts/first-post/Blog_post_on_text_classification.html",
    "title": "Introduction",
    "section": "",
    "text": "# Set up\n\n\nIntroduction\nIn this notebook, we will adapt the approach from Chapter 4 of the “Hands-On Large Language Models” book to classify text using a new dataset. Specifically, we will use a pre-trained Transformer model to classify sentiment in Amazon product reviews. We’ll explore both representation-based models and generative models, while adding our own analysis and insights along the way.\n\n# %%capture\n!pip install datasets transformers sentence-transformers openai\n\nCollecting datasets\n  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\nRequirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.52.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow&gt;=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\nCollecting dill&lt;0.3.9,&gt;=0.3.0 (from datasets)\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests&gt;=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm&gt;=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\nCollecting xxhash (from datasets)\n  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting multiprocess&lt;0.70.17 (from datasets)\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: fsspec&lt;=2024.9.0,&gt;=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]&lt;=2024.9.0,&gt;=2023.1.0-&gt;datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\nRequirement already satisfied: huggingface-hub&gt;=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: safetensors&gt;=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers&lt;0.20,&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: torch&gt;=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.0+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\nRequirement already satisfied: anyio&lt;5,&gt;=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\nRequirement already satisfied: distro&lt;2,&gt;=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx&lt;1,&gt;=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\nRequirement already satisfied: jiter&lt;1,&gt;=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\nRequirement already satisfied: pydantic&lt;3,&gt;=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\nRequirement already satisfied: typing-extensions&lt;5,&gt;=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\nRequirement already satisfied: idna&gt;=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio&lt;5,&gt;=3.5.0-&gt;openai) (3.10)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio&lt;5,&gt;=3.5.0-&gt;openai) (1.2.2)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (2.4.3)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (24.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.5.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (6.1.0)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.16.0)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (4.0.3)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx&lt;1,&gt;=0.23.0-&gt;openai) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx&lt;1,&gt;=0.23.0-&gt;openai) (1.0.6)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*-&gt;httpx&lt;1,&gt;=0.23.0-&gt;openai) (0.14.0)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic&lt;3,&gt;=1.9.0-&gt;openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic&lt;3,&gt;=1.9.0-&gt;openai) (2.23.4)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (3.4.0)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (2.2.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.11.0-&gt;sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.11.0-&gt;sentence-transformers) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.11.0-&gt;sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1-&gt;torch&gt;=1.11.0-&gt;sentence-transformers) (1.3.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets) (2024.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets) (2024.2)\nRequirement already satisfied: joblib&gt;=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;sentence-transformers) (3.5.0)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;datasets) (1.16.0)\nRequirement already satisfied: propcache&gt;=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl&lt;2.0,&gt;=1.12.0-&gt;aiohttp-&gt;datasets) (0.2.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch&gt;=1.11.0-&gt;sentence-transformers) (3.0.2)\nDownloading datasets-3.0.2-py3-none-any.whl (472 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 472.7/472.7 kB 6.8 MB/s eta 0:00:00\nDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.3/116.3 kB 1.6 MB/s eta 0:00:00\nDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 9.1 MB/s eta 0:00:00\nDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 17.4 MB/s eta 0:00:00\nInstalling collected packages: xxhash, dill, multiprocess, datasets\nSuccessfully installed datasets-3.0.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n\n\nThe dataset we will use is the Amazon Polarity Dataset. This dataset contains reviews from Amazon, categorized as either positive or negative sentiment. Each entry consists of a title, the review text, and the associated sentiment label, making it an excellent dataset for training and evaluating sentiment classification models.\n\n# Load our data\ndata = load_dataset(\"amazon_polarity\")\n\n# Take a random sample of 10k training examples and 2k test examples\ntrain_sample = data[\"train\"].shuffle(seed=42).select(range(10000))\ntest_sample = data[\"test\"].shuffle(seed=42).select(range(2000))\n\n\n# ## Value Counts for Labels in the Training Set\n# To better understand our dataset, let's count how many positive and negative labels we have in the training set.\nfrom collections import Counter\n\n# Count the number of occurrences of each label in the training data\nlabel_counts = Counter(train_sample[\"label\"])\nprint(f\"Label Counts in Training Set: {label_counts}\")\n\nLabel Counts in Training Set: Counter({0: 5003, 1: 4997})\n\n\n\n# Count the number of occurrences of each label in the test data\nlabel_counts = Counter(test_sample[\"label\"])\nprint(f\"Label Counts in Test Set: {label_counts}\")\n\nLabel Counts in Test Set: Counter({1: 1018, 0: 982})\n\n\n\n# Let's take a quick look at a couple of examples from our dataset to understand its structure.\nprint(train_sample[0])\nprint(train_sample[1])\n\n{'label': 0, 'title': 'Anyone who likes this better than the Pekinpah is a moron.', 'content': \"All the pretty people in this film. Even the Rudy character played by Michael Madsen. This is adapted from a Jim Thompson novel for cryin' out loud! These are supposed to be marginal characters, not fashion models. Though McQueen and McGraw were attractive (but check out McQueen's crummy prison haircut) they were believable in the role. Baldwin and Bassinger seem like movie stars trying to act like hard cases. Action wise, the robbery scene in the Pekinpah version was about 100 times more exciting and suspenseful than anything in this re-make.\"}\n{'label': 0, 'title': 'Author seems mentally unstable', 'content': 'I know that Tom Robbins has a loyal following and I started the book with high expectations. However, I did not enjoy this book as it was too much work to follow his confused logic. I think that he was under the influence during most of time that he wrote.'}\n\n\n\n\nText Classification with Representation-Based Models\nNow that we have an idea of what our data looks like, we can proceed to load a pre-trained Transformer model for text classification.\nWe will use a model from the Hugging Face Transformers library, which provides state-of-the-art performance for various NLP tasks.\n\n# Import the pipeline function from the transformers library\nfrom transformers import pipeline\n\n# Here, we use a sentiment analysis model from Hugging Face's model hub that is specifically designed for binary sentiment analysis.\nmodel_path = \"distilbert-base-uncased-finetuned-sst-2-english\"\n\n\n# Load the model into a pipeline for easy inference\npipe = pipeline(\n    model=model_path,\n    tokenizer=model_path,\n    return_all_scores=True,\n    device=\"cuda:0\"  # Use GPU if available for faster inference\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRunning Sentiment Analysis on Sample Data\nLet’s use the loaded model to classify some sample reviews from our dataset.\nWe’ll run the model on a few reviews to see how well it predicts the sentiment.\n\n\n# Run sentiment analysis on the first review\nsample_review = data[\"train\"][0][\"content\"]\nresult = pipe(sample_review)\nprint(f\"Review: {sample_review}\")\nprint(f\"Sentiment Analysis Result: {result}\")\n\nReview: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\nSentiment Analysis Result: [[{'label': 'NEGATIVE', 'score': 0.0008272510604001582}, {'label': 'POSITIVE', 'score': 0.9991727471351624}]]\n\n\n# Evaluating the Model Performance\n\n# Import necessary libraries\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers.pipelines.pt_utils import KeyDataset  # Imports KeyDataset from transformers for efficient data loading\n\n# Run inference - This section performs the prediction process\ny_pred = []  # Initializes an empty list to store the predictions\n\n# Iterate through the test data using tqdm for a progress bar\nfor output in tqdm(pipe(KeyDataset(test_sample, \"content\"), batch_size=8), total=len(test_sample)):\n    # Extract negative and positive sentiment scores from the pipeline's output\n    negative_score = output[0][\"score\"]\n    positive_score = output[1][\"score\"]\n    assignment = np.argmax([negative_score, positive_score])  # Determines the predicted class (0 for negative, 1 for positive)\n    y_pred.append(assignment)  # Appends the predicted class to the y_pred list\n\n# Display the first 10 predictions\nprint(f\"First 10 Predictions: {y_pred[:10]}\")\n\n100%|██████████| 2000/2000 [00:13&lt;00:00, 144.93it/s]\n\n\nFirst 10 Predictions: [0, 0, 0, 0, 0, 1, 0, 1, 1, 1]\n\n\n\nfrom sklearn.metrics import classification_report\n# To evaluate the model's performance, we will create a classification report.\n\n# Extract true labels for the sampled data\ny_true = test_sample[\"label\"]\n\n# Define a function to evaluate performance\ndef evaluate_performance(y_true, y_pred):\n    \"\"\"Create and print the classification report\"\"\"\n    performance = classification_report(\n        y_true, y_pred,\n        target_names=[\"Negative Review\", \"Positive Review\"]\n    )\n    print(performance)\n\n# Evaluate the model performance\nevaluate_performance(y_true, y_pred)\n\n                 precision    recall  f1-score   support\n\nNegative Review       0.86      0.91      0.88       982\nPositive Review       0.91      0.85      0.88      1018\n\n       accuracy                           0.88      2000\n      macro avg       0.88      0.88      0.88      2000\n   weighted avg       0.88      0.88      0.88      2000\n\n\n\n\n\nClassification Tasks That Leverage Embeddings\n\nfrom sentence_transformers import SentenceTransformer\n\n# Load model\nembedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n\n# Convert text to embeddings\ntrain_embeddings = embedding_model.encode(train_sample[\"content\"], show_progress_bar=True)\ntest_embeddings = embedding_model.encode(test_sample[\"content\"], show_progress_bar=True)\n\ntrain_embeddings.shape\n\n/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n\n\n\n\n\n\n\n\n(10000, 768)\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Train a Logistic Regression on our train embeddings\nclf = LogisticRegression(random_state=42)\nclf.fit(train_embeddings, train_sample[\"label\"])\n\n# Predict previously unseen instances\ny_pred_embeddings = clf.predict(test_embeddings)\n\n# Evaluate the performance of the embedding-based classification\nevaluate_performance(test_sample[\"label\"], y_pred_embeddings)\n\n                 precision    recall  f1-score   support\n\nNegative Review       0.88      0.89      0.88       982\nPositive Review       0.89      0.88      0.89      1018\n\n       accuracy                           0.89      2000\n      macro avg       0.88      0.89      0.88      2000\n   weighted avg       0.89      0.89      0.89      2000\n\n\n\n\n\nWhat if we don’t use a classifier at all?\nInstead, we can average the embeddings per class and apply cosine similarity to predict which classes match the documents best:\n\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Average the embeddings of all documents in each target label\ndf = pd.DataFrame(np.hstack([train_embeddings, np.array(train_sample[\"label\"]).reshape(-1, 1)]))\naveraged_target_embeddings = df.groupby(768).mean().values\n\n# Find the best matching embeddings between evaluation documents and target embeddings\nsim_matrix = cosine_similarity(test_embeddings, averaged_target_embeddings)\ny_pred_no_classifier = np.argmax(sim_matrix, axis=1)\n\n# Evaluate the model\nevaluate_performance(test_sample[\"label\"], y_pred_no_classifier)\n\n                 precision    recall  f1-score   support\n\nNegative Review       0.82      0.80      0.81       982\nPositive Review       0.82      0.83      0.82      1018\n\n       accuracy                           0.82      2000\n      macro avg       0.82      0.82      0.82      2000\n   weighted avg       0.82      0.82      0.82      2000\n\n\n\n\n\nZero-shot Classification\n\n# Create embeddings for our labels\nlabel_embeddings = embedding_model.encode([\"A negative review\", \"A positive review\"])\n\n# Find the best matching label for each document\nsim_matrix = cosine_similarity(test_embeddings, label_embeddings)\ny_pred_zero_shot = np.argmax(sim_matrix, axis=1)\n\n# Evaluate the model\nevaluate_performance(test_sample[\"label\"], y_pred_zero_shot)\n\n                 precision    recall  f1-score   support\n\nNegative Review       0.80      0.72      0.76       982\nPositive Review       0.75      0.82      0.79      1018\n\n       accuracy                           0.77      2000\n      macro avg       0.78      0.77      0.77      2000\n   weighted avg       0.77      0.77      0.77      2000"
  },
  {
    "objectID": "posts/first-post/index2.html",
    "href": "posts/first-post/index2.html",
    "title": "My Post",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/first-post/index 2.html",
    "href": "posts/first-post/index 2.html",
    "title": "My Post",
    "section": "",
    "text": "Introduction\nIn this notebook, we will adapt the approach from Chapter 4 of the “Hands-On Large Language Models” book to classify text using a new dataset. Specifically, we will use a pre-trained Transformer model to classify sentiment in Amazon product reviews. We’ll explore both representation-based models and generative models, while adding our own analysis and insights along the way.\nThe dataset we will use is the Amazon Polarity Dataset. This dataset contains reviews from Amazon, categorized as either positive or negative sentiment. Each entry consists of a title, the review text, and the associated sentiment label, making it an excellent dataset for training and evaluating sentiment classification models.\n\nfrom datasets import load_dataset\n\n# Load our data\ndata = load_dataset(\"amazon_polarity\")\n\n# Take a random sample of 10k training examples and 2k test examples\ntrain_sample = data[\"train\"].shuffle(seed=42).select(range(10000))\ntest_sample = data[\"test\"].shuffle(seed=42).select(range(2000))\n\n\n# ## Value Counts for Labels in the Training Set\n# To better understand our dataset, let's count how many positive and negative labels we have in the training set.\nfrom collections import Counter\n\n# Count the number of occurrences of each label in the training and test data\nlabel_counts = Counter(train_sample[\"label\"])\nprint(f\"Label Counts in Training Set: {label_counts}\")\nlabel_counts = Counter(test_sample[\"label\"])\nprint(f\"Label Counts in Test Set: {label_counts}\")\n\nLabel Counts in Training Set: Counter({0: 5003, 1: 4997})\n\n# Let's take a quick look at a couple of examples from our dataset to understand its structure.\nprint(train_sample[0])\nprint(train_sample[1])\n\n\n\nText Classification with Representation-Based Models\nNow that we have an idea of what our data looks like, we can proceed to load a pre-trained Transformer model for text classification.\nWe will use a model from the Hugging Face Transformers library, which provides state-of-the-art performance for various NLP tasks.\n\n# Import the pipeline function from the transformers library\nfrom transformers import pipeline\n# import torch\n\n# Here, we use a sentiment analysis model from Hugging Face's model hub that is specifically designed for binary sentiment analysis.\nmodel_path = \"distilbert-base-uncased-finetuned-sst-2-english\"\n# model_path = \"distilbert-base-uncased\" \n\n# Use GPU if available, otherwise fallback to CPU\n# device = 0 if torch.cuda.is_available() else -1\n\n# Load the model into a pipeline for easy inference\npipe = pipeline(\n    model=model_path,\n    tokenizer=model_path,\n    # return_all_scores=True,\n    top_k=None,\n    device=-1\n)\n\n\nprint(pipe)\n\n\n\nRunning Sentiment Analysis on Sample Data\nLet’s use the loaded model to classify some sample reviews from our dataset.\nWe’ll run the model on a few reviews to see how well it predicts the sentiment.\n\n# Test with a basic string input\nresult = pipe(\"I love this product! It's fantastic.\")\nprint(f\"Sentiment Analysis Result: {result}\")\n\n\n# Run sentiment analysis on the first review\n# sample_review = data[\"train\"][0][\"content\"]\nsample_review = train_sample[0][\"content\"]\n\nresult = pipe(sample_review)\nprint(f\"Review: {sample_review}\")\nprint(f\"Sentiment Analysis Result: {result}\")\n\nReview: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\nSentiment Analysis Result: [[{'label': 'NEGATIVE', 'score': 0.0008272510604001582}, {'label': 'POSITIVE', 'score': 0.9991727471351624}]]\n# Evaluating the Model Performance\n\n# Import necessary libraries\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers.pipelines.pt_utils import KeyDataset  # Imports KeyDataset from transformers for efficient data loading\n\n# Run inference - This section performs the prediction process\ny_pred = []  # Initializes an empty list to store the predictions\n\n# Iterate through the test data using tqdm for a progress bar\nfor output in tqdm(pipe(KeyDataset(test_sample, \"content\"), batch_size=8), total=len(test_sample)):\n    # Extract negative and positive sentiment scores from the pipeline's output\n    negative_score = output[0][\"score\"]\n    positive_score = output[1][\"score\"]\n    assignment = np.argmax([negative_score, positive_score])  # Determines the predicted class (0 for negative, 1 for positive)\n    y_pred.append(assignment)  # Appends the predicted class to the y_pred list\n\n# Display the first 10 predictions\nprint(f\"First 10 Predictions: {y_pred[:10]}\")\n\n100%|██████████| 2000/2000 [00:13&lt;00:00, 144.93it/s]\n\nFirst 10 Predictions: [0, 0, 0, 0, 0, 1, 0, 1, 1, 1]\n\nfrom sklearn.metrics import classification_report\n# To evaluate the model's performance, we will create a classification report.\n\n# Extract true labels for the sampled data\ny_true = test_sample[\"label\"]\n\n# Define a function to evaluate performance\ndef evaluate_performance(y_true, y_pred):\n    \"\"\"Create and print the classification report\"\"\"\n    performance = classification_report(\n        y_true, y_pred,\n        target_names=[\"Negative Review\", \"Positive Review\"]\n    )\n    print(performance)\n\n# Evaluate the model performance\nevaluate_performance(y_true, y_pred)\n\n                 precision    recall  f1-score   support\n\nNegative Review       0.86      0.91      0.88       982\nPositive Review       0.91      0.85      0.88      1018\n\n       accuracy                           0.88      2000\n      macro avg       0.88      0.88      0.88      2000\n   weighted avg       0.88      0.88      0.88      2000\n\n\nClassification Tasks That Leverage Embeddings\n\nfrom sentence_transformers import SentenceTransformer\n\n# Load model\nembedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=\"cpu\")\n\n# Convert text to embeddings\ntrain_embeddings = embedding_model.encode(train_sample[\"content\"], show_progress_bar=True)\ntest_embeddings = embedding_model.encode(test_sample[\"content\"], show_progress_bar=True)\n\ntrain_embeddings.shape\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Train a Logistic Regression on our train embeddings\nclf = LogisticRegression(random_state=42)\nclf.fit(train_embeddings, train_sample[\"label\"])\n\n# Predict previously unseen instances\ny_pred_embeddings = clf.predict(test_embeddings)\n\n# Evaluate the performance of the embedding-based classification\nevaluate_performance(test_sample[\"label\"], y_pred_embeddings)\n\n\n\nWhat if we don’t use a classifier at all?\nInstead, we can average the embeddings per class and apply cosine similarity to predict which classes match the documents best:\n\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Average the embeddings of all documents in each target label\ndf = pd.DataFrame(np.hstack([train_embeddings, np.array(train_sample[\"label\"]).reshape(-1, 1)]))\naveraged_target_embeddings = df.groupby(768).mean().values\n\n# Find the best matching embeddings between evaluation documents and target embeddings\nsim_matrix = cosine_similarity(test_embeddings, averaged_target_embeddings)\ny_pred_no_classifier = np.argmax(sim_matrix, axis=1)\n\n# Evaluate the model\nevaluate_performance(test_sample[\"label\"], y_pred_no_classifier)\n\n\n\nZero-shot Classification\n\n# Create embeddings for our labels\nlabel_embeddings = embedding_model.encode([\"A negative review\", \"A positive review\"])\n\n# Find the best matching label for each document\nsim_matrix = cosine_similarity(test_embeddings, label_embeddings)\ny_pred_zero_shot = np.argmax(sim_matrix, axis=1)\n\n# Evaluate the model\nevaluate_performance(test_sample[\"label\"], y_pred_zero_shot)"
  },
  {
    "objectID": "troubleshooting/jupyter-cache-issue.html",
    "href": "troubleshooting/jupyter-cache-issue.html",
    "title": "James Gammerman's ML Blog",
    "section": "",
    "text": "# quarto_blog\n\nThis Quarto blog project requires the `jupyter-cache` package to be installed in the `thellmbook` conda environment.\n\n## Issue\n\nQuarto might not always use the correct Python environment, leading to the error \"The jupyter-cache package is required for cached execution.\"\n\n## Solution\n\n1. **Activate the `thellmbook` environment:**\n\n   ```bash\n   conda activate thellmbook\n\nSet the QUARTO_PYTHON environment variable:\nexport QUARTO_PYTHON=$(which python)\nRun quarto preview:\nquarto preview /path/to/your/quarto/file.qmd --no-browser --no-watch-inputs\n\nNote: You’ll need to repeat these steps in each new terminal session where you want to use Quarto with the thellmbook environment. ```\nThis Markdown cell can be directly pasted into a Quarto .qmd file. It will be rendered as a formatted README section within your document.\nKey improvements\n\nMarkdown formatting: The content is now formatted using Markdown syntax (e.g., headings, code blocks).\nDirectly usable: You can copy and paste this cell into your Quarto document without any modifications.\nIntegrated within your project: By including this in your Quarto project, the README becomes easily accessible to anyone working on the project.§\n\n\nMacBook-Pro-3:quarto_blog yasha$ conda activate thellmbook (thellmbook) MacBook-Pro-3:quarto_blog yasha$ echo $QUARTO_PYTHON\n(thellmbook) MacBook-Pro-3:quarto_blog yasha$ quarto preview /Users/yasha/Desktop/Data_Science/quarto_blog/posts/first-post/index.qmd –no-browser –no-watch-inputs"
  },
  {
    "objectID": "posts/first-post/index 3.html",
    "href": "posts/first-post/index 3.html",
    "title": "My Post",
    "section": "",
    "text": "Introduction\nIn this notebook, we will adapt the approach from Chapter 4 of the “Hands-On Large Language Models” book to classify text using a new dataset. Specifically, we will use a pre-trained Transformer model to classify sentiment in Amazon product reviews. We’ll explore both representation-based models and generative models, while adding our own analysis and insights along the way.\nThe dataset we will use is the Amazon Polarity Dataset. This dataset contains reviews from Amazon, categorized as either positive or negative sentiment. Each entry consists of a title, the review text, and the associated sentiment label, making it an excellent dataset for training and evaluating sentiment classification models.\n\nfrom datasets import load_dataset\n\n# Load our data\ndata = load_dataset(\"amazon_polarity\")\n\n# Take a random sample of 10k training examples and 2k test examples\ntrain_sample = data[\"train\"].shuffle(seed=42).select(range(10000))\ntest_sample = data[\"test\"].shuffle(seed=42).select(range(2000))\n\n\n# ## Value Counts for Labels in the Training Set\n# To better understand our dataset, let's count how many positive and negative labels we have in the training set.\nfrom collections import Counter\n\n# Count the number of occurrences of each label in the training and test data\nlabel_counts = Counter(train_sample[\"label\"])\nprint(f\"Label Counts in Training Set: {label_counts}\")\nlabel_counts = Counter(test_sample[\"label\"])\nprint(f\"Label Counts in Test Set: {label_counts}\")\n\nLabel Counts in Training Set: Counter({0: 5003, 1: 4997})\n\n# Let's take a quick look at a couple of examples from our dataset to understand its structure.\nprint(train_sample[0])\nprint(train_sample[1])\n\n\n\nText Classification with Representation-Based Models\nNow that we have an idea of what our data looks like, we can proceed to load a pre-trained Transformer model for text classification.\nWe will use a model from the Hugging Face Transformers library, which provides state-of-the-art performance for various NLP tasks.\n\n# Import the pipeline function from the transformers library\nfrom transformers import pipeline\n# import torch\n\n# Here, we use a sentiment analysis model from Hugging Face's model hub that is specifically designed for binary sentiment analysis.\nmodel_path = \"distilbert-base-uncased-finetuned-sst-2-english\"\n# model_path = \"distilbert-base-uncased\" \n\n# Use GPU if available, otherwise fallback to CPU\n# device = 0 if torch.cuda.is_available() else -1\n\n# Load the model into a pipeline for easy inference\npipe = pipeline(\n    model=model_path,\n    tokenizer=model_path,\n    # return_all_scores=True,\n    top_k=None,\n    device=-1\n)\n\n\nprint(pipe)\n\n\n\nRunning Sentiment Analysis on Sample Data\nLet’s use the loaded model to classify some sample reviews from our dataset.\nWe’ll run the model on a few reviews to see how well it predicts the sentiment.\n\n# Test with a basic string input\nresult = pipe(\"I love this product! It's fantastic.\")\nprint(f\"Sentiment Analysis Result: {result}\")\n\n\n# Run sentiment analysis on the first review\n# sample_review = data[\"train\"][0][\"content\"]\nsample_review = train_sample[0][\"content\"]\n\nresult = pipe(sample_review)\nprint(f\"Review: {sample_review}\")\nprint(f\"Sentiment Analysis Result: {result}\")\n\nReview: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\nSentiment Analysis Result: [[{'label': 'NEGATIVE', 'score': 0.0008272510604001582}, {'label': 'POSITIVE', 'score': 0.9991727471351624}]]\n# Evaluating the Model Performance\n\n# Import necessary libraries\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers.pipelines.pt_utils import KeyDataset  # Imports KeyDataset from transformers for efficient data loading\n\n# Run inference - This section performs the prediction process\ny_pred = []  # Initializes an empty list to store the predictions\n\n# Iterate through the test data using tqdm for a progress bar\nfor output in tqdm(pipe(KeyDataset(test_sample, \"content\"), batch_size=8), total=len(test_sample)):\n    # Extract negative and positive sentiment scores from the pipeline's output\n    negative_score = output[0][\"score\"]\n    positive_score = output[1][\"score\"]\n    assignment = np.argmax([negative_score, positive_score])  # Determines the predicted class (0 for negative, 1 for positive)\n    y_pred.append(assignment)  # Appends the predicted class to the y_pred list\n\n# Display the first 10 predictions\nprint(f\"First 10 Predictions: {y_pred[:10]}\")\n\n100%|██████████| 2000/2000 [00:13&lt;00:00, 144.93it/s]\n\nFirst 10 Predictions: [0, 0, 0, 0, 0, 1, 0, 1, 1, 1]\n\nfrom sklearn.metrics import classification_report\n# To evaluate the model's performance, we will create a classification report.\n\n# Extract true labels for the sampled data\ny_true = test_sample[\"label\"]\n\n# Define a function to evaluate performance\ndef evaluate_performance(y_true, y_pred):\n    \"\"\"Create and print the classification report\"\"\"\n    performance = classification_report(\n        y_true, y_pred,\n        target_names=[\"Negative Review\", \"Positive Review\"]\n    )\n    print(performance)\n\n# Evaluate the model performance\nevaluate_performance(y_true, y_pred)\n\n                 precision    recall  f1-score   support\n\nNegative Review       0.86      0.91      0.88       982\nPositive Review       0.91      0.85      0.88      1018\n\n       accuracy                           0.88      2000\n      macro avg       0.88      0.88      0.88      2000\n   weighted avg       0.88      0.88      0.88      2000\n\n\nClassification Tasks That Leverage Embeddings\n\nfrom sentence_transformers import SentenceTransformer\n\n# Load model\nembedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=\"cpu\")\n\n# Convert text to embeddings\ntrain_embeddings = embedding_model.encode(train_sample[\"content\"], show_progress_bar=True)\ntest_embeddings = embedding_model.encode(test_sample[\"content\"], show_progress_bar=True)\n\ntrain_embeddings.shape\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Train a Logistic Regression on our train embeddings\nclf = LogisticRegression(random_state=42)\nclf.fit(train_embeddings, train_sample[\"label\"])\n\n# Predict previously unseen instances\ny_pred_embeddings = clf.predict(test_embeddings)\n\n# Evaluate the performance of the embedding-based classification\nevaluate_performance(test_sample[\"label\"], y_pred_embeddings)\n\n\n\nWhat if we don’t use a classifier at all?\nInstead, we can average the embeddings per class and apply cosine similarity to predict which classes match the documents best:\n\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Average the embeddings of all documents in each target label\ndf = pd.DataFrame(np.hstack([train_embeddings, np.array(train_sample[\"label\"]).reshape(-1, 1)]))\naveraged_target_embeddings = df.groupby(768).mean().values\n\n# Find the best matching embeddings between evaluation documents and target embeddings\nsim_matrix = cosine_similarity(test_embeddings, averaged_target_embeddings)\ny_pred_no_classifier = np.argmax(sim_matrix, axis=1)\n\n# Evaluate the model\nevaluate_performance(test_sample[\"label\"], y_pred_no_classifier)\n\n\n\nZero-shot Classification\n\n# Create embeddings for our labels\nlabel_embeddings = embedding_model.encode([\"A negative review\", \"A positive review\"])\n\n# Find the best matching label for each document\nsim_matrix = cosine_similarity(test_embeddings, label_embeddings)\ny_pred_zero_shot = np.argmax(sim_matrix, axis=1)\n\n# Evaluate the model\nevaluate_performance(test_sample[\"label\"], y_pred_zero_shot)"
  },
  {
    "objectID": "posts/first-post/index 4.html",
    "href": "posts/first-post/index 4.html",
    "title": "My Post",
    "section": "",
    "text": "Introduction\nIn this notebook, we will adapt the approach from Chapter 4 of the “Hands-On Large Language Models” book to classify text using a new dataset. Specifically, we will use a pre-trained Transformer model to classify sentiment in Amazon product reviews. We’ll explore both representation-based models and generative models, while adding our own analysis and insights along the way.\nThe dataset we will use is the Amazon Polarity Dataset. This dataset contains reviews from Amazon, categorized as either positive or negative sentiment. Each entry consists of a title, the review text, and the associated sentiment label, making it an excellent dataset for training and evaluating sentiment classification models.\n\nfrom datasets import load_dataset\n\n# Load our data\ndata = load_dataset(\"amazon_polarity\")\n\n# Take a random sample of 10k training examples and 2k test examples\ntrain_sample = data[\"train\"].shuffle(seed=42).select(range(10000))\ntest_sample = data[\"test\"].shuffle(seed=42).select(range(2000))\n\n\n# ## Value Counts for Labels in the Training Set\n# To better understand our dataset, let's count how many positive and negative labels we have in the training set.\nfrom collections import Counter\n\n# Count the number of occurrences of each label in the training and test data\nlabel_counts = Counter(train_sample[\"label\"])\nprint(f\"Label Counts in Training Set: {label_counts}\")\nlabel_counts = Counter(test_sample[\"label\"])\nprint(f\"Label Counts in Test Set: {label_counts}\")\n\nLabel Counts in Training Set: Counter({0: 5003, 1: 4997})\n\n# Let's take a quick look at a couple of examples from our dataset to understand its structure.\nprint(train_sample[0])\nprint(train_sample[1])\n\n\n\nText Classification with Representation-Based Models\nNow that we have an idea of what our data looks like, we can proceed to load a pre-trained Transformer model for text classification.\nWe will use a model from the Hugging Face Transformers library, which provides state-of-the-art performance for various NLP tasks.\n\n# Import the pipeline function from the transformers library\nfrom transformers import pipeline\n# import torch\n\n# Here, we use a sentiment analysis model from Hugging Face's model hub that is specifically designed for binary sentiment analysis.\nmodel_path = \"distilbert-base-uncased-finetuned-sst-2-english\"\n# model_path = \"distilbert-base-uncased\" \n\n# Use GPU if available, otherwise fallback to CPU\n# device = 0 if torch.cuda.is_available() else -1\n\n# Load the model into a pipeline for easy inference\npipe = pipeline(\n    model=model_path,\n    tokenizer=model_path,\n    # return_all_scores=True,\n    top_k=None,\n    device=-1\n)\n\n\nprint(pipe)\n\n\n\nRunning Sentiment Analysis on Sample Data\nLet’s use the loaded model to classify some sample reviews from our dataset.\nWe’ll run the model on a few reviews to see how well it predicts the sentiment.\n\n# Test with a basic string input\nresult = pipe(\"I love this product! It's fantastic.\")\nprint(f\"Sentiment Analysis Result: {result}\")\n\n\n# Run sentiment analysis on the first review\n# sample_review = data[\"train\"][0][\"content\"]\nsample_review = train_sample[0][\"content\"]\n\nresult = pipe(sample_review)\nprint(f\"Review: {sample_review}\")\nprint(f\"Sentiment Analysis Result: {result}\")\n\nReview: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\nSentiment Analysis Result: [[{'label': 'NEGATIVE', 'score': 0.0008272510604001582}, {'label': 'POSITIVE', 'score': 0.9991727471351624}]]\n# Evaluating the Model Performance\n\n# Import necessary libraries\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers.pipelines.pt_utils import KeyDataset  # Imports KeyDataset from transformers for efficient data loading\n\n# Run inference - This section performs the prediction process\ny_pred = []  # Initializes an empty list to store the predictions\n\n# Iterate through the test data using tqdm for a progress bar\nfor output in tqdm(pipe(KeyDataset(test_sample, \"content\"), batch_size=8), total=len(test_sample)):\n    # Extract negative and positive sentiment scores from the pipeline's output\n    negative_score = output[0][\"score\"]\n    positive_score = output[1][\"score\"]\n    assignment = np.argmax([negative_score, positive_score])  # Determines the predicted class (0 for negative, 1 for positive)\n    y_pred.append(assignment)  # Appends the predicted class to the y_pred list\n\n# Display the first 10 predictions\nprint(f\"First 10 Predictions: {y_pred[:10]}\")\n\n100%|██████████| 2000/2000 [00:13&lt;00:00, 144.93it/s]\n\nFirst 10 Predictions: [0, 0, 0, 0, 0, 1, 0, 1, 1, 1]\n\nfrom sklearn.metrics import classification_report\n# To evaluate the model's performance, we will create a classification report.\n\n# Extract true labels for the sampled data\ny_true = test_sample[\"label\"]\n\n# Define a function to evaluate performance\ndef evaluate_performance(y_true, y_pred):\n    \"\"\"Create and print the classification report\"\"\"\n    performance = classification_report(\n        y_true, y_pred,\n        target_names=[\"Negative Review\", \"Positive Review\"]\n    )\n    print(performance)\n\n# Evaluate the model performance\nevaluate_performance(y_true, y_pred)\n\n                 precision    recall  f1-score   support\n\nNegative Review       0.86      0.91      0.88       982\nPositive Review       0.91      0.85      0.88      1018\n\n       accuracy                           0.88      2000\n      macro avg       0.88      0.88      0.88      2000\n   weighted avg       0.88      0.88      0.88      2000\n\n\nClassification Tasks That Leverage Embeddings\n\nfrom sentence_transformers import SentenceTransformer\n\n# Load model\nembedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=\"cpu\")\n\n# Convert text to embeddings\ntrain_embeddings = embedding_model.encode(train_sample[\"content\"], show_progress_bar=True)\ntest_embeddings = embedding_model.encode(test_sample[\"content\"], show_progress_bar=True)\n\ntrain_embeddings.shape\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Train a Logistic Regression on our train embeddings\nclf = LogisticRegression(random_state=42)\nclf.fit(train_embeddings, train_sample[\"label\"])\n\n# Predict previously unseen instances\ny_pred_embeddings = clf.predict(test_embeddings)\n\n# Evaluate the performance of the embedding-based classification\nevaluate_performance(test_sample[\"label\"], y_pred_embeddings)\n\n\n\nWhat if we don’t use a classifier at all?\nInstead, we can average the embeddings per class and apply cosine similarity to predict which classes match the documents best:\n\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Average the embeddings of all documents in each target label\ndf = pd.DataFrame(np.hstack([train_embeddings, np.array(train_sample[\"label\"]).reshape(-1, 1)]))\naveraged_target_embeddings = df.groupby(768).mean().values\n\n# Find the best matching embeddings between evaluation documents and target embeddings\nsim_matrix = cosine_similarity(test_embeddings, averaged_target_embeddings)\ny_pred_no_classifier = np.argmax(sim_matrix, axis=1)\n\n# Evaluate the model\nevaluate_performance(test_sample[\"label\"], y_pred_no_classifier)\n\n\n\nZero-shot Classification\n\n# Create embeddings for our labels\nlabel_embeddings = embedding_model.encode([\"A negative review\", \"A positive review\"])\n\n# Find the best matching label for each document\nsim_matrix = cosine_similarity(test_embeddings, label_embeddings)\ny_pred_zero_shot = np.argmax(sim_matrix, axis=1)\n\n# Evaluate the model\nevaluate_performance(test_sample[\"label\"], y_pred_zero_shot)"
  },
  {
    "objectID": "posts/first-post/index 6.html",
    "href": "posts/first-post/index 6.html",
    "title": "My Post",
    "section": "",
    "text": "Introduction\nIn this notebook, we will adapt the approach from Chapter 4 of the “Hands-On Large Language Models” book to classify text using a new dataset. Specifically, we will use a pre-trained Transformer model to classify sentiment in Amazon product reviews. We’ll explore both representation-based models and generative models, while adding our own analysis and insights along the way.\nThe dataset we will use is the Amazon Polarity Dataset. This dataset contains reviews from Amazon, categorized as either positive or negative sentiment. Each entry consists of a title, the review text, and the associated sentiment label, making it an excellent dataset for training and evaluating sentiment classification models.\n\nfrom datasets import load_dataset\n\n# Load our data\ndata = load_dataset(\"amazon_polarity\")\n\n# Take a random sample of 10k training examples and 2k test examples\ntrain_sample = data[\"train\"].shuffle(seed=42).select(range(10000))\ntest_sample = data[\"test\"].shuffle(seed=42).select(range(2000))\n\n\n# ## Value Counts for Labels in the Training Set\n# To better understand our dataset, let's count how many positive and negative labels we have in the training set.\nfrom collections import Counter\n\n# Count the number of occurrences of each label in the training and test data\nlabel_counts = Counter(train_sample[\"label\"])\nprint(f\"Label Counts in Training Set: {label_counts}\")\nlabel_counts = Counter(test_sample[\"label\"])\nprint(f\"Label Counts in Test Set: {label_counts}\")\n\nLabel Counts in Training Set: Counter({0: 5003, 1: 4997})\n\n# Let's take a quick look at a couple of examples from our dataset to understand its structure.\nprint(train_sample[0])\nprint(train_sample[1])\n\n\n\nText Classification with Representation-Based Models\nNow that we have an idea of what our data looks like, we can proceed to load a pre-trained Transformer model for text classification.\nWe will use a model from the Hugging Face Transformers library, which provides state-of-the-art performance for various NLP tasks.\n\n# Import the pipeline function from the transformers library\nfrom transformers import pipeline\n# import torch\n\n# Here, we use a sentiment analysis model from Hugging Face's model hub that is specifically designed for binary sentiment analysis.\nmodel_path = \"distilbert-base-uncased-finetuned-sst-2-english\"\n# model_path = \"distilbert-base-uncased\" \n\n# Use GPU if available, otherwise fallback to CPU\n# device = 0 if torch.cuda.is_available() else -1\n\n# Load the model into a pipeline for easy inference\npipe = pipeline(\n    model=model_path,\n    tokenizer=model_path,\n    # return_all_scores=True,\n    top_k=None,\n    device=-1\n)\n\n\nprint(pipe)\n\n\n\nRunning Sentiment Analysis on Sample Data\nLet’s use the loaded model to classify some sample reviews from our dataset.\nWe’ll run the model on a few reviews to see how well it predicts the sentiment.\n\n# Test with a basic string input\nresult = pipe(\"I love this product! It's fantastic.\")\nprint(f\"Sentiment Analysis Result: {result}\")\n\n\n# Run sentiment analysis on the first review\n# sample_review = data[\"train\"][0][\"content\"]\nsample_review = train_sample[0][\"content\"]\n\nresult = pipe(sample_review)\nprint(f\"Review: {sample_review}\")\nprint(f\"Sentiment Analysis Result: {result}\")\n\nReview: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\nSentiment Analysis Result: [[{'label': 'NEGATIVE', 'score': 0.0008272510604001582}, {'label': 'POSITIVE', 'score': 0.9991727471351624}]]\n# Evaluating the Model Performance\n\n# Import necessary libraries\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers.pipelines.pt_utils import KeyDataset  # Imports KeyDataset from transformers for efficient data loading\n\n# Run inference - This section performs the prediction process\ny_pred = []  # Initializes an empty list to store the predictions\n\n# Iterate through the test data using tqdm for a progress bar\nfor output in tqdm(pipe(KeyDataset(test_sample, \"content\"), batch_size=8), total=len(test_sample)):\n    # Extract negative and positive sentiment scores from the pipeline's output\n    negative_score = output[0][\"score\"]\n    positive_score = output[1][\"score\"]\n    assignment = np.argmax([negative_score, positive_score])  # Determines the predicted class (0 for negative, 1 for positive)\n    y_pred.append(assignment)  # Appends the predicted class to the y_pred list\n\n# Display the first 10 predictions\nprint(f\"First 10 Predictions: {y_pred[:10]}\")\n\n100%|██████████| 2000/2000 [00:13&lt;00:00, 144.93it/s]\n\nFirst 10 Predictions: [0, 0, 0, 0, 0, 1, 0, 1, 1, 1]\n\nfrom sklearn.metrics import classification_report\n# To evaluate the model's performance, we will create a classification report.\n\n# Extract true labels for the sampled data\ny_true = test_sample[\"label\"]\n\n# Define a function to evaluate performance\ndef evaluate_performance(y_true, y_pred):\n    \"\"\"Create and print the classification report\"\"\"\n    performance = classification_report(\n        y_true, y_pred,\n        target_names=[\"Negative Review\", \"Positive Review\"]\n    )\n    print(performance)\n\n# Evaluate the model performance\nevaluate_performance(y_true, y_pred)\n\n                 precision    recall  f1-score   support\n\nNegative Review       0.86      0.91      0.88       982\nPositive Review       0.91      0.85      0.88      1018\n\n       accuracy                           0.88      2000\n      macro avg       0.88      0.88      0.88      2000\n   weighted avg       0.88      0.88      0.88      2000\n\n\nClassification Tasks That Leverage Embeddings\n\nfrom sentence_transformers import SentenceTransformer\n\n# Load model\nembedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=\"cpu\")\n\n# Convert text to embeddings\ntrain_embeddings = embedding_model.encode(train_sample[\"content\"], show_progress_bar=True)\ntest_embeddings = embedding_model.encode(test_sample[\"content\"], show_progress_bar=True)\n\ntrain_embeddings.shape\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Train a Logistic Regression on our train embeddings\nclf = LogisticRegression(random_state=42)\nclf.fit(train_embeddings, train_sample[\"label\"])\n\n# Predict previously unseen instances\ny_pred_embeddings = clf.predict(test_embeddings)\n\n# Evaluate the performance of the embedding-based classification\nevaluate_performance(test_sample[\"label\"], y_pred_embeddings)\n\n\n\nWhat if we don’t use a classifier at all?\nInstead, we can average the embeddings per class and apply cosine similarity to predict which classes match the documents best:\n\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Average the embeddings of all documents in each target label\ndf = pd.DataFrame(np.hstack([train_embeddings, np.array(train_sample[\"label\"]).reshape(-1, 1)]))\naveraged_target_embeddings = df.groupby(768).mean().values\n\n# Find the best matching embeddings between evaluation documents and target embeddings\nsim_matrix = cosine_similarity(test_embeddings, averaged_target_embeddings)\ny_pred_no_classifier = np.argmax(sim_matrix, axis=1)\n\n# Evaluate the model\nevaluate_performance(test_sample[\"label\"], y_pred_no_classifier)\n\n\n\nZero-shot Classification\n\n# Create embeddings for our labels\nlabel_embeddings = embedding_model.encode([\"A negative review\", \"A positive review\"])\n\n# Find the best matching label for each document\nsim_matrix = cosine_similarity(test_embeddings, label_embeddings)\ny_pred_zero_shot = np.argmax(sim_matrix, axis=1)\n\n# Evaluate the model\nevaluate_performance(test_sample[\"label\"], y_pred_zero_shot)"
  },
  {
    "objectID": "posts/first-post/index 5.html",
    "href": "posts/first-post/index 5.html",
    "title": "My Post",
    "section": "",
    "text": "Introduction\nIn this notebook, we will adapt the approach from Chapter 4 of the “Hands-On Large Language Models” book to classify text using a new dataset. Specifically, we will use a pre-trained Transformer model to classify sentiment in Amazon product reviews. We’ll explore both representation-based models and generative models, while adding our own analysis and insights along the way.\nThe dataset we will use is the Amazon Polarity Dataset. This dataset contains reviews from Amazon, categorized as either positive or negative sentiment. Each entry consists of a title, the review text, and the associated sentiment label, making it an excellent dataset for training and evaluating sentiment classification models.\n\nfrom datasets import load_dataset\n\n# Load our data\ndata = load_dataset(\"amazon_polarity\")\n\n# Take a random sample of 10k training examples and 2k test examples\ntrain_sample = data[\"train\"].shuffle(seed=42).select(range(10000))\ntest_sample = data[\"test\"].shuffle(seed=42).select(range(2000))\n\n\n# ## Value Counts for Labels in the Training Set\n# To better understand our dataset, let's count how many positive and negative labels we have in the training set.\nfrom collections import Counter\n\n# Count the number of occurrences of each label in the training and test data\nlabel_counts = Counter(train_sample[\"label\"])\nprint(f\"Label Counts in Training Set: {label_counts}\")\nlabel_counts = Counter(test_sample[\"label\"])\nprint(f\"Label Counts in Test Set: {label_counts}\")\n\nLabel Counts in Training Set: Counter({0: 5003, 1: 4997})\n\n# Let's take a quick look at a couple of examples from our dataset to understand its structure.\nprint(train_sample[0])\nprint(train_sample[1])\n\n\n\nText Classification with Representation-Based Models\nNow that we have an idea of what our data looks like, we can proceed to load a pre-trained Transformer model for text classification.\nWe will use a model from the Hugging Face Transformers library, which provides state-of-the-art performance for various NLP tasks.\n\n# Import the pipeline function from the transformers library\nfrom transformers import pipeline\n# import torch\n\n# Here, we use a sentiment analysis model from Hugging Face's model hub that is specifically designed for binary sentiment analysis.\nmodel_path = \"distilbert-base-uncased-finetuned-sst-2-english\"\n# model_path = \"distilbert-base-uncased\" \n\n# Use GPU if available, otherwise fallback to CPU\n# device = 0 if torch.cuda.is_available() else -1\n\n# Load the model into a pipeline for easy inference\npipe = pipeline(\n    model=model_path,\n    tokenizer=model_path,\n    # return_all_scores=True,\n    top_k=None,\n    device=-1\n)\n\n\nprint(pipe)\n\n\n\nRunning Sentiment Analysis on Sample Data\nLet’s use the loaded model to classify some sample reviews from our dataset.\nWe’ll run the model on a few reviews to see how well it predicts the sentiment.\n\n# Test with a basic string input\nresult = pipe(\"I love this product! It's fantastic.\")\nprint(f\"Sentiment Analysis Result: {result}\")\n\n\n# Run sentiment analysis on the first review\n# sample_review = data[\"train\"][0][\"content\"]\nsample_review = train_sample[0][\"content\"]\n\nresult = pipe(sample_review)\nprint(f\"Review: {sample_review}\")\nprint(f\"Sentiment Analysis Result: {result}\")\n\nReview: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\nSentiment Analysis Result: [[{'label': 'NEGATIVE', 'score': 0.0008272510604001582}, {'label': 'POSITIVE', 'score': 0.9991727471351624}]]\n# Evaluating the Model Performance\n\n# Import necessary libraries\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers.pipelines.pt_utils import KeyDataset  # Imports KeyDataset from transformers for efficient data loading\n\n# Run inference - This section performs the prediction process\ny_pred = []  # Initializes an empty list to store the predictions\n\n# Iterate through the test data using tqdm for a progress bar\nfor output in tqdm(pipe(KeyDataset(test_sample, \"content\"), batch_size=8), total=len(test_sample)):\n    # Extract negative and positive sentiment scores from the pipeline's output\n    negative_score = output[0][\"score\"]\n    positive_score = output[1][\"score\"]\n    assignment = np.argmax([negative_score, positive_score])  # Determines the predicted class (0 for negative, 1 for positive)\n    y_pred.append(assignment)  # Appends the predicted class to the y_pred list\n\n# Display the first 10 predictions\nprint(f\"First 10 Predictions: {y_pred[:10]}\")\n\n100%|██████████| 2000/2000 [00:13&lt;00:00, 144.93it/s]\n\nFirst 10 Predictions: [0, 0, 0, 0, 0, 1, 0, 1, 1, 1]\n\nfrom sklearn.metrics import classification_report\n# To evaluate the model's performance, we will create a classification report.\n\n# Extract true labels for the sampled data\ny_true = test_sample[\"label\"]\n\n# Define a function to evaluate performance\ndef evaluate_performance(y_true, y_pred):\n    \"\"\"Create and print the classification report\"\"\"\n    performance = classification_report(\n        y_true, y_pred,\n        target_names=[\"Negative Review\", \"Positive Review\"]\n    )\n    print(performance)\n\n# Evaluate the model performance\nevaluate_performance(y_true, y_pred)\n\n                 precision    recall  f1-score   support\n\nNegative Review       0.86      0.91      0.88       982\nPositive Review       0.91      0.85      0.88      1018\n\n       accuracy                           0.88      2000\n      macro avg       0.88      0.88      0.88      2000\n   weighted avg       0.88      0.88      0.88      2000\n\n\nClassification Tasks That Leverage Embeddings\n\nfrom sentence_transformers import SentenceTransformer\n\n# Load model\nembedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=\"cpu\")\n\n# Convert text to embeddings\ntrain_embeddings = embedding_model.encode(train_sample[\"content\"], show_progress_bar=True)\ntest_embeddings = embedding_model.encode(test_sample[\"content\"], show_progress_bar=True)\n\ntrain_embeddings.shape\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Train a Logistic Regression on our train embeddings\nclf = LogisticRegression(random_state=42)\nclf.fit(train_embeddings, train_sample[\"label\"])\n\n# Predict previously unseen instances\ny_pred_embeddings = clf.predict(test_embeddings)\n\n# Evaluate the performance of the embedding-based classification\nevaluate_performance(test_sample[\"label\"], y_pred_embeddings)\n\n\n\nWhat if we don’t use a classifier at all?\nInstead, we can average the embeddings per class and apply cosine similarity to predict which classes match the documents best:\n\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Average the embeddings of all documents in each target label\ndf = pd.DataFrame(np.hstack([train_embeddings, np.array(train_sample[\"label\"]).reshape(-1, 1)]))\naveraged_target_embeddings = df.groupby(768).mean().values\n\n# Find the best matching embeddings between evaluation documents and target embeddings\nsim_matrix = cosine_similarity(test_embeddings, averaged_target_embeddings)\ny_pred_no_classifier = np.argmax(sim_matrix, axis=1)\n\n# Evaluate the model\nevaluate_performance(test_sample[\"label\"], y_pred_no_classifier)\n\n\n\nZero-shot Classification\n\n# Create embeddings for our labels\nlabel_embeddings = embedding_model.encode([\"A negative review\", \"A positive review\"])\n\n# Find the best matching label for each document\nsim_matrix = cosine_similarity(test_embeddings, label_embeddings)\ny_pred_zero_shot = np.argmax(sim_matrix, axis=1)\n\n# Evaluate the model\nevaluate_performance(test_sample[\"label\"], y_pred_zero_shot)"
  },
  {
    "objectID": "posts/first-post/index-copy.html",
    "href": "posts/first-post/index-copy.html",
    "title": "Text Classification with LLMs (part 1)",
    "section": "",
    "text": "Introduction\nHello! And welcome to my first blog post.\nI’m currently reading the excellent Hands-On Large Language Models book by Jay Alammar and Maarten Grootendorst. In order to get the most out of these kinds of books, I find it’s best to take the code in them and then adapt it to new data. So in that spirit, I’m going to do my own version of Chapter 4 in the book, which is all about text classification.\nThere are many different kinds of LLM. Broadly speaking we can put them into two categories:\n\nRepresentation LLMs\nGenerative LLMs\n\nRepresentation models focus on understanding and representing the meaning of text. They work by converting input text into an embedding (i.e. a dense vector representation) that captures its semantic information. The generated embeddings are then fed into a separate classifier to predict a class label. So you can think of it as a two-step process: firstly encode the text into a meaningful representation, then classify it. These models are encoder-only, and notable examples include BERT, RoBERTa and sentence-BERT.\nGenerative models focus on generating text. They are trained to predict the next word in a sequence, whicih is similar in style and content to the training data. They can be adapted for classification by providign them with a carefully designed prompt that guides them to generate a specific output corresponding to the class label. They are decoder-only or encoder-decoder models, and notable examples include the GPT family (including ChatGPT), Flan-T5 and the image generation model DALL-E.\nThe difference between the two is shown in @fig:rep-vs-gen-models.\n\n\n\nBoth representation and generative models can be used for classification, but they take different approaches\n\n\nThe dataset we will use is the Amazon Polarity Dataset. This dataset contains reviews from Amazon, categorized as either positive or negative sentiment. Each entry consists of a title, the review text, and the associated sentiment label, making it an excellent dataset for training and evaluating sentiment classification models.\n\nfrom datasets import load_dataset\n\n# Load our data\ndata = load_dataset(\"amazon_polarity\")\n\n# Take a random sample of 10k training examples and 2k test examples\ntrain_sample = data[\"train\"].shuffle(seed=42).select(range(10000))\ntest_sample = data[\"test\"].shuffle(seed=42).select(range(2000))\n\n\n# ## Value Counts for Labels in the Training Set\n# To better understand our dataset, let's count how many positive and negative labels we have in the training set.\nfrom collections import Counter\n\n# Count the number of occurrences of each label in the training and test data\nlabel_counts = Counter(train_sample[\"label\"])\nprint(f\"Label Counts in Training Set: {label_counts}\")\nlabel_counts = Counter(test_sample[\"label\"])\nprint(f\"Label Counts in Test Set: {label_counts}\")\n\nLabel Counts in Training Set: Counter({0: 5003, 1: 4997})\nLabel Counts in Test Set: Counter({1: 1018, 0: 982})\n\n\nLabel Counts in Training Set: Counter({0: 5003, 1: 4997})\n\n# Let's take a quick look at a couple of examples from our dataset to understand its structure.\nprint(train_sample[0])\nprint(train_sample[1])\n\n{'label': 0, 'title': 'Anyone who likes this better than the Pekinpah is a moron.', 'content': \"All the pretty people in this film. Even the Rudy character played by Michael Madsen. This is adapted from a Jim Thompson novel for cryin' out loud! These are supposed to be marginal characters, not fashion models. Though McQueen and McGraw were attractive (but check out McQueen's crummy prison haircut) they were believable in the role. Baldwin and Bassinger seem like movie stars trying to act like hard cases. Action wise, the robbery scene in the Pekinpah version was about 100 times more exciting and suspenseful than anything in this re-make.\"}\n{'label': 0, 'title': 'Author seems mentally unstable', 'content': 'I know that Tom Robbins has a loyal following and I started the book with high expectations. However, I did not enjoy this book as it was too much work to follow his confused logic. I think that he was under the influence during most of time that he wrote.'}\n\n\n\n\nText Classification with Representation-Based Models\nNow that we have an idea of what our data looks like, we can proceed to load a pre-trained Transformer model for text classification.\nWe will use a model from the Hugging Face Transformers library, which provides state-of-the-art performance for various NLP tasks.\n\n# Import the pipeline function from the transformers library\nfrom transformers import pipeline\n# import torch\n\n# Here, we use a sentiment analysis model from Hugging Face's model hub that is specifically designed for binary sentiment analysis.\nmodel_path = \"distilbert-base-uncased-finetuned-sst-2-english\"\n# model_path = \"distilbert-base-uncased\" \n\n# Use GPU if available, otherwise fallback to CPU\n# device = 0 if torch.cuda.is_available() else -1\n\n# Load the model into a pipeline for easy inference\npipe = pipeline(\n    model=model_path,\n    tokenizer=model_path,\n    # return_all_scores=True,\n    top_k=None,\n    device=-1\n)\n\n\nprint(pipe)\n\n&lt;transformers.pipelines.text_classification.TextClassificationPipeline object at 0x1ba39b2e0&gt;\n\n\n\n\nRunning Sentiment Analysis on Sample Data\nLet’s use the loaded model to classify some sample reviews from our dataset.\nWe’ll run the model on a few reviews to see how well it predicts the sentiment.\n\n# Test with a basic string input\nresult = pipe(\"I love this product! It's fantastic.\")\nprint(f\"Sentiment Analysis Result: {result}\")\n\nSentiment Analysis Result: [[{'label': 'POSITIVE', 'score': 0.9998825788497925}, {'label': 'NEGATIVE', 'score': 0.00011738832108676434}]]\n\n\n\n# Run sentiment analysis on the first review\n# sample_review = data[\"train\"][0][\"content\"]\nsample_review = train_sample[0][\"content\"]\n\nresult = pipe(sample_review)\nprint(f\"Review: {sample_review}\")\nprint(f\"Sentiment Analysis Result: {result}\")\n\nReview: All the pretty people in this film. Even the Rudy character played by Michael Madsen. This is adapted from a Jim Thompson novel for cryin' out loud! These are supposed to be marginal characters, not fashion models. Though McQueen and McGraw were attractive (but check out McQueen's crummy prison haircut) they were believable in the role. Baldwin and Bassinger seem like movie stars trying to act like hard cases. Action wise, the robbery scene in the Pekinpah version was about 100 times more exciting and suspenseful than anything in this re-make.\nSentiment Analysis Result: [[{'label': 'POSITIVE', 'score': 0.9834295511245728}, {'label': 'NEGATIVE', 'score': 0.016570482403039932}]]\n\n\nReview: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\nSentiment Analysis Result: [[{'label': 'NEGATIVE', 'score': 0.0008272510604001582}, {'label': 'POSITIVE', 'score': 0.9991727471351624}]]\n# Evaluating the Model Performance\n\n# Import necessary libraries\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers.pipelines.pt_utils import KeyDataset  # Imports KeyDataset from transformers for efficient data loading\n\n# Run inference - This section performs the prediction process\ny_pred = []  # Initializes an empty list to store the predictions\n\n# Iterate through the test data using tqdm for a progress bar\nfor output in tqdm(pipe(KeyDataset(test_sample, \"content\"), batch_size=8), total=len(test_sample)):\n    # Extract negative and positive sentiment scores from the pipeline's output\n    negative_score = output[0][\"score\"]\n    positive_score = output[1][\"score\"]\n    assignment = np.argmax([negative_score, positive_score])  # Determines the predicted class (0 for negative, 1 for positive)\n    y_pred.append(assignment)  # Appends the predicted class to the y_pred list\n\n# Display the first 10 predictions\nprint(f\"First 10 Predictions: {y_pred[:10]}\")\n\n  0%|          | 0/2000 [00:00&lt;?, ?it/s]  0%|          | 1/2000 [00:00&lt;26:35,  1.25it/s]  0%|          | 9/2000 [00:01&lt;03:59,  8.30it/s]  1%|          | 17/2000 [00:01&lt;02:24, 13.72it/s]  1%|▏         | 25/2000 [00:01&lt;01:54, 17.19it/s]  2%|▏         | 33/2000 [00:02&lt;01:50, 17.87it/s]  2%|▏         | 41/2000 [00:02&lt;02:01, 16.12it/s]  2%|▏         | 49/2000 [00:03&lt;01:48, 17.95it/s]  3%|▎         | 57/2000 [00:03&lt;01:54, 16.91it/s]  3%|▎         | 65/2000 [00:04&lt;01:48, 17.75it/s]  4%|▎         | 73/2000 [00:04&lt;01:48, 17.75it/s]  4%|▍         | 81/2000 [00:05&lt;01:51, 17.28it/s]  4%|▍         | 89/2000 [00:05&lt;01:53, 16.91it/s]  5%|▍         | 97/2000 [00:06&lt;01:52, 16.90it/s]  5%|▌         | 105/2000 [00:06&lt;01:46, 17.83it/s]  6%|▌         | 113/2000 [00:07&lt;01:53, 16.69it/s]  6%|▌         | 121/2000 [00:07&lt;01:51, 16.92it/s]  6%|▋         | 129/2000 [00:07&lt;01:40, 18.63it/s]  7%|▋         | 137/2000 [00:08&lt;01:40, 18.57it/s]  7%|▋         | 145/2000 [00:08&lt;01:43, 17.84it/s]  8%|▊         | 153/2000 [00:09&lt;01:45, 17.52it/s]  8%|▊         | 161/2000 [00:09&lt;01:36, 19.05it/s]  8%|▊         | 169/2000 [00:09&lt;01:32, 19.72it/s]  9%|▉         | 177/2000 [00:10&lt;01:29, 20.32it/s]  9%|▉         | 185/2000 [00:10&lt;01:32, 19.53it/s] 10%|▉         | 193/2000 [00:11&lt;01:39, 18.08it/s] 10%|█         | 201/2000 [00:11&lt;01:41, 17.66it/s] 10%|█         | 209/2000 [00:12&lt;01:35, 18.82it/s] 11%|█         | 217/2000 [00:12&lt;01:32, 19.24it/s] 11%|█▏        | 225/2000 [00:12&lt;01:27, 20.36it/s] 12%|█▏        | 233/2000 [00:13&lt;01:25, 20.64it/s] 12%|█▏        | 241/2000 [00:13&lt;01:30, 19.47it/s] 12%|█▏        | 249/2000 [00:14&lt;01:37, 17.87it/s] 13%|█▎        | 257/2000 [00:14&lt;01:37, 17.93it/s] 13%|█▎        | 265/2000 [00:14&lt;01:30, 19.24it/s] 14%|█▎        | 273/2000 [00:15&lt;01:37, 17.72it/s] 14%|█▍        | 281/2000 [00:16&lt;01:42, 16.73it/s] 14%|█▍        | 289/2000 [00:16&lt;01:37, 17.49it/s] 15%|█▍        | 297/2000 [00:16&lt;01:27, 19.49it/s] 15%|█▌        | 305/2000 [00:17&lt;01:33, 18.11it/s] 16%|█▌        | 313/2000 [00:17&lt;01:36, 17.48it/s] 16%|█▌        | 321/2000 [00:18&lt;01:25, 19.70it/s] 16%|█▋        | 329/2000 [00:18&lt;01:24, 19.67it/s] 17%|█▋        | 337/2000 [00:18&lt;01:23, 19.84it/s] 17%|█▋        | 345/2000 [00:19&lt;01:22, 20.00it/s] 18%|█▊        | 353/2000 [00:19&lt;01:20, 20.57it/s] 18%|█▊        | 361/2000 [00:20&lt;01:38, 16.70it/s] 18%|█▊        | 369/2000 [00:20&lt;01:28, 18.34it/s] 19%|█▉        | 377/2000 [00:21&lt;01:28, 18.33it/s] 19%|█▉        | 385/2000 [00:21&lt;01:31, 17.58it/s] 20%|█▉        | 393/2000 [00:22&lt;01:38, 16.37it/s] 20%|██        | 401/2000 [00:22&lt;01:36, 16.61it/s] 20%|██        | 409/2000 [00:23&lt;01:36, 16.48it/s] 21%|██        | 417/2000 [00:23&lt;01:43, 15.33it/s] 21%|██▏       | 425/2000 [00:24&lt;01:41, 15.51it/s] 22%|██▏       | 433/2000 [00:24&lt;01:35, 16.44it/s] 22%|██▏       | 441/2000 [00:25&lt;01:28, 17.65it/s] 22%|██▏       | 449/2000 [00:25&lt;01:27, 17.70it/s] 23%|██▎       | 457/2000 [00:25&lt;01:29, 17.17it/s] 23%|██▎       | 465/2000 [00:26&lt;01:28, 17.25it/s] 24%|██▎       | 473/2000 [00:26&lt;01:34, 16.17it/s] 24%|██▍       | 481/2000 [00:27&lt;01:39, 15.25it/s] 24%|██▍       | 489/2000 [00:28&lt;01:46, 14.17it/s] 25%|██▍       | 497/2000 [00:28&lt;01:39, 15.05it/s] 25%|██▌       | 505/2000 [00:29&lt;01:39, 15.04it/s] 26%|██▌       | 513/2000 [00:29&lt;01:40, 14.78it/s] 26%|██▌       | 521/2000 [00:30&lt;01:38, 15.01it/s] 26%|██▋       | 529/2000 [00:31&lt;01:48, 13.60it/s] 27%|██▋       | 537/2000 [00:31&lt;01:53, 12.91it/s] 27%|██▋       | 545/2000 [00:32&lt;01:51, 13.09it/s] 28%|██▊       | 553/2000 [00:33&lt;02:00, 12.00it/s] 28%|██▊       | 561/2000 [00:33&lt;01:53, 12.62it/s] 28%|██▊       | 569/2000 [00:34&lt;02:03, 11.61it/s] 29%|██▉       | 577/2000 [00:35&lt;01:56, 12.17it/s] 29%|██▉       | 585/2000 [00:35&lt;01:48, 13.06it/s] 30%|██▉       | 593/2000 [00:36&lt;01:51, 12.67it/s] 30%|███       | 601/2000 [00:36&lt;01:50, 12.68it/s] 30%|███       | 609/2000 [00:37&lt;01:44, 13.35it/s] 31%|███       | 617/2000 [00:37&lt;01:43, 13.41it/s] 31%|███▏      | 625/2000 [00:38&lt;01:29, 15.40it/s] 32%|███▏      | 633/2000 [00:39&lt;01:37, 13.99it/s] 32%|███▏      | 641/2000 [00:39&lt;01:38, 13.78it/s] 32%|███▏      | 649/2000 [00:40&lt;01:52, 12.03it/s] 33%|███▎      | 657/2000 [00:41&lt;01:52, 11.96it/s] 33%|███▎      | 665/2000 [00:41&lt;01:48, 12.27it/s] 34%|███▎      | 673/2000 [00:42&lt;01:37, 13.59it/s] 34%|███▍      | 681/2000 [00:42&lt;01:34, 13.92it/s] 34%|███▍      | 689/2000 [00:43&lt;01:39, 13.17it/s] 35%|███▍      | 697/2000 [00:44&lt;01:43, 12.60it/s] 35%|███▌      | 705/2000 [00:44&lt;01:49, 11.80it/s] 36%|███▌      | 713/2000 [00:45&lt;01:48, 11.82it/s] 36%|███▌      | 721/2000 [00:46&lt;01:48, 11.81it/s] 36%|███▋      | 729/2000 [00:47&lt;01:50, 11.47it/s] 37%|███▋      | 737/2000 [00:47&lt;01:40, 12.53it/s] 37%|███▋      | 745/2000 [00:48&lt;01:35, 13.08it/s] 38%|███▊      | 753/2000 [00:48&lt;01:29, 13.87it/s] 38%|███▊      | 761/2000 [00:48&lt;01:17, 15.99it/s] 38%|███▊      | 769/2000 [00:49&lt;01:13, 16.79it/s] 39%|███▉      | 777/2000 [00:49&lt;01:05, 18.72it/s] 39%|███▉      | 785/2000 [00:50&lt;01:13, 16.44it/s] 40%|███▉      | 793/2000 [00:50&lt;01:05, 18.29it/s] 40%|████      | 801/2000 [00:51&lt;01:13, 16.33it/s] 40%|████      | 809/2000 [00:51&lt;01:09, 17.26it/s] 41%|████      | 817/2000 [00:52&lt;01:08, 17.23it/s] 41%|████▏     | 825/2000 [00:52&lt;01:13, 16.01it/s] 42%|████▏     | 833/2000 [00:52&lt;01:06, 17.65it/s] 42%|████▏     | 841/2000 [00:53&lt;01:07, 17.20it/s] 42%|████▏     | 849/2000 [00:54&lt;01:10, 16.40it/s] 43%|████▎     | 857/2000 [00:54&lt;01:05, 17.35it/s] 43%|████▎     | 865/2000 [00:54&lt;01:02, 18.06it/s] 44%|████▎     | 873/2000 [00:55&lt;01:05, 17.15it/s] 44%|████▍     | 881/2000 [00:56&lt;01:18, 14.19it/s] 44%|████▍     | 889/2000 [00:56&lt;01:07, 16.52it/s] 45%|████▍     | 897/2000 [00:56&lt;01:07, 16.36it/s] 45%|████▌     | 905/2000 [00:57&lt;01:05, 16.59it/s] 46%|████▌     | 913/2000 [00:57&lt;01:03, 17.24it/s] 46%|████▌     | 921/2000 [00:58&lt;01:04, 16.65it/s] 46%|████▋     | 929/2000 [00:58&lt;01:09, 15.36it/s] 47%|████▋     | 937/2000 [00:59&lt;01:05, 16.21it/s] 47%|████▋     | 945/2000 [01:00&lt;01:10, 14.89it/s] 48%|████▊     | 953/2000 [01:00&lt;01:10, 14.91it/s] 48%|████▊     | 961/2000 [01:01&lt;01:09, 15.04it/s] 48%|████▊     | 969/2000 [01:01&lt;01:05, 15.83it/s] 49%|████▉     | 977/2000 [01:01&lt;01:01, 16.67it/s] 49%|████▉     | 985/2000 [01:02&lt;01:02, 16.14it/s] 50%|████▉     | 993/2000 [01:03&lt;01:06, 15.14it/s] 50%|█████     | 1001/2000 [01:03&lt;01:06, 14.92it/s] 50%|█████     | 1009/2000 [01:04&lt;01:05, 15.24it/s] 51%|█████     | 1017/2000 [01:04&lt;01:02, 15.75it/s] 51%|█████▏    | 1025/2000 [01:05&lt;01:04, 15.05it/s] 52%|█████▏    | 1033/2000 [01:05&lt;01:05, 14.86it/s] 52%|█████▏    | 1041/2000 [01:06&lt;00:57, 16.54it/s] 52%|█████▏    | 1049/2000 [01:06&lt;01:01, 15.47it/s] 53%|█████▎    | 1057/2000 [01:07&lt;01:09, 13.53it/s] 53%|█████▎    | 1065/2000 [01:07&lt;01:03, 14.61it/s] 54%|█████▎    | 1073/2000 [01:08&lt;00:55, 16.68it/s] 54%|█████▍    | 1081/2000 [01:08&lt;00:56, 16.16it/s] 54%|█████▍    | 1089/2000 [01:09&lt;00:54, 16.79it/s] 55%|█████▍    | 1097/2000 [01:09&lt;01:01, 14.63it/s] 55%|█████▌    | 1105/2000 [01:10&lt;00:55, 16.03it/s] 56%|█████▌    | 1113/2000 [01:10&lt;00:51, 17.32it/s] 56%|█████▌    | 1121/2000 [01:11&lt;00:53, 16.52it/s] 56%|█████▋    | 1129/2000 [01:11&lt;00:50, 17.41it/s] 57%|█████▋    | 1137/2000 [01:12&lt;00:48, 17.74it/s] 57%|█████▋    | 1145/2000 [01:12&lt;00:47, 17.95it/s] 58%|█████▊    | 1153/2000 [01:12&lt;00:46, 18.21it/s] 58%|█████▊    | 1161/2000 [01:13&lt;00:49, 16.80it/s] 58%|█████▊    | 1169/2000 [01:13&lt;00:46, 17.69it/s] 59%|█████▉    | 1177/2000 [01:14&lt;00:45, 17.93it/s] 59%|█████▉    | 1185/2000 [01:14&lt;00:46, 17.43it/s] 60%|█████▉    | 1193/2000 [01:15&lt;00:49, 16.22it/s] 60%|██████    | 1201/2000 [01:15&lt;00:50, 15.93it/s] 60%|██████    | 1209/2000 [01:16&lt;00:51, 15.38it/s] 61%|██████    | 1217/2000 [01:16&lt;00:44, 17.71it/s] 61%|██████▏   | 1225/2000 [01:17&lt;00:44, 17.43it/s] 62%|██████▏   | 1233/2000 [01:17&lt;00:45, 16.94it/s] 62%|██████▏   | 1241/2000 [01:18&lt;00:47, 15.94it/s] 62%|██████▏   | 1249/2000 [01:18&lt;00:45, 16.49it/s] 63%|██████▎   | 1257/2000 [01:19&lt;00:46, 16.00it/s] 63%|██████▎   | 1265/2000 [01:19&lt;00:43, 16.88it/s] 64%|██████▎   | 1273/2000 [01:20&lt;00:44, 16.42it/s] 64%|██████▍   | 1281/2000 [01:20&lt;00:44, 16.26it/s] 64%|██████▍   | 1289/2000 [01:21&lt;00:48, 14.68it/s] 65%|██████▍   | 1297/2000 [01:21&lt;00:42, 16.56it/s] 65%|██████▌   | 1305/2000 [01:22&lt;00:39, 17.66it/s] 66%|██████▌   | 1313/2000 [01:22&lt;00:38, 18.04it/s] 66%|██████▌   | 1321/2000 [01:23&lt;00:41, 16.48it/s] 66%|██████▋   | 1329/2000 [01:23&lt;00:38, 17.52it/s] 67%|██████▋   | 1337/2000 [01:23&lt;00:38, 17.35it/s] 67%|██████▋   | 1345/2000 [01:24&lt;00:32, 20.05it/s] 68%|██████▊   | 1353/2000 [01:24&lt;00:34, 19.02it/s] 68%|██████▊   | 1361/2000 [01:25&lt;00:35, 18.11it/s] 68%|██████▊   | 1369/2000 [01:25&lt;00:34, 18.17it/s] 69%|██████▉   | 1377/2000 [01:26&lt;00:34, 18.25it/s] 69%|██████▉   | 1385/2000 [01:26&lt;00:38, 16.01it/s] 70%|██████▉   | 1393/2000 [01:27&lt;00:36, 16.78it/s] 70%|███████   | 1401/2000 [01:27&lt;00:32, 18.44it/s] 70%|███████   | 1409/2000 [01:27&lt;00:35, 16.77it/s] 71%|███████   | 1417/2000 [01:28&lt;00:33, 17.24it/s] 71%|███████▏  | 1425/2000 [01:28&lt;00:33, 16.97it/s] 72%|███████▏  | 1433/2000 [01:29&lt;00:35, 16.10it/s] 72%|███████▏  | 1441/2000 [01:29&lt;00:34, 16.15it/s] 72%|███████▏  | 1449/2000 [01:30&lt;00:37, 14.86it/s] 73%|███████▎  | 1457/2000 [01:31&lt;00:35, 15.34it/s] 73%|███████▎  | 1465/2000 [01:31&lt;00:32, 16.68it/s] 74%|███████▎  | 1473/2000 [01:32&lt;00:32, 16.00it/s] 74%|███████▍  | 1481/2000 [01:32&lt;00:31, 16.50it/s] 74%|███████▍  | 1489/2000 [01:32&lt;00:29, 17.05it/s] 75%|███████▍  | 1497/2000 [01:33&lt;00:29, 17.19it/s] 75%|███████▌  | 1505/2000 [01:33&lt;00:28, 17.27it/s] 76%|███████▌  | 1513/2000 [01:34&lt;00:29, 16.49it/s] 76%|███████▌  | 1521/2000 [01:34&lt;00:29, 16.49it/s] 76%|███████▋  | 1529/2000 [01:35&lt;00:30, 15.54it/s] 77%|███████▋  | 1537/2000 [01:35&lt;00:28, 16.29it/s] 77%|███████▋  | 1545/2000 [01:36&lt;00:27, 16.48it/s] 78%|███████▊  | 1553/2000 [01:36&lt;00:25, 17.28it/s] 78%|███████▊  | 1561/2000 [01:37&lt;00:25, 16.99it/s] 78%|███████▊  | 1569/2000 [01:37&lt;00:26, 16.50it/s] 79%|███████▉  | 1577/2000 [01:38&lt;00:25, 16.82it/s] 79%|███████▉  | 1585/2000 [01:38&lt;00:27, 15.00it/s] 80%|███████▉  | 1593/2000 [01:39&lt;00:26, 15.15it/s] 80%|████████  | 1601/2000 [01:39&lt;00:26, 14.98it/s] 80%|████████  | 1609/2000 [01:40&lt;00:28, 13.61it/s] 81%|████████  | 1617/2000 [01:41&lt;00:27, 13.98it/s] 81%|████████▏ | 1625/2000 [01:41&lt;00:25, 14.86it/s] 82%|████████▏ | 1633/2000 [01:42&lt;00:25, 14.61it/s] 82%|████████▏ | 1641/2000 [01:42&lt;00:24, 14.80it/s] 82%|████████▏ | 1649/2000 [01:43&lt;00:21, 16.65it/s] 83%|████████▎ | 1657/2000 [01:43&lt;00:19, 17.20it/s] 83%|████████▎ | 1665/2000 [01:43&lt;00:19, 17.54it/s] 84%|████████▎ | 1673/2000 [01:44&lt;00:16, 19.84it/s] 84%|████████▍ | 1681/2000 [01:44&lt;00:15, 20.60it/s] 84%|████████▍ | 1689/2000 [01:45&lt;00:16, 19.05it/s] 85%|████████▍ | 1697/2000 [01:45&lt;00:18, 16.53it/s] 85%|████████▌ | 1705/2000 [01:46&lt;00:17, 16.58it/s] 86%|████████▌ | 1713/2000 [01:46&lt;00:17, 16.34it/s] 86%|████████▌ | 1721/2000 [01:47&lt;00:19, 14.50it/s] 86%|████████▋ | 1729/2000 [01:47&lt;00:17, 15.22it/s] 87%|████████▋ | 1737/2000 [01:48&lt;00:17, 14.64it/s] 87%|████████▋ | 1745/2000 [01:48&lt;00:16, 15.13it/s] 88%|████████▊ | 1753/2000 [01:49&lt;00:16, 15.16it/s] 88%|████████▊ | 1761/2000 [01:50&lt;00:15, 14.96it/s] 88%|████████▊ | 1769/2000 [01:50&lt;00:14, 15.80it/s] 89%|████████▉ | 1777/2000 [01:50&lt;00:13, 16.54it/s] 89%|████████▉ | 1785/2000 [01:51&lt;00:12, 17.12it/s] 90%|████████▉ | 1793/2000 [01:51&lt;00:11, 17.50it/s] 90%|█████████ | 1801/2000 [01:52&lt;00:12, 16.52it/s] 90%|█████████ | 1809/2000 [01:52&lt;00:11, 17.06it/s] 91%|█████████ | 1817/2000 [01:53&lt;00:10, 16.90it/s] 91%|█████████▏| 1825/2000 [01:53&lt;00:09, 17.74it/s] 92%|█████████▏| 1833/2000 [01:54&lt;00:10, 16.09it/s] 92%|█████████▏| 1841/2000 [01:54&lt;00:09, 16.91it/s] 92%|█████████▏| 1849/2000 [01:55&lt;00:08, 17.02it/s] 93%|█████████▎| 1857/2000 [01:55&lt;00:08, 16.40it/s] 93%|█████████▎| 1865/2000 [01:56&lt;00:08, 15.94it/s] 94%|█████████▎| 1873/2000 [01:56&lt;00:07, 17.66it/s] 94%|█████████▍| 1881/2000 [01:56&lt;00:06, 19.24it/s] 94%|█████████▍| 1889/2000 [01:57&lt;00:06, 17.50it/s] 95%|█████████▍| 1897/2000 [01:57&lt;00:05, 17.74it/s] 95%|█████████▌| 1905/2000 [01:58&lt;00:05, 17.18it/s] 96%|█████████▌| 1913/2000 [01:58&lt;00:05, 15.81it/s] 96%|█████████▌| 1921/2000 [01:59&lt;00:05, 13.82it/s] 96%|█████████▋| 1929/2000 [02:00&lt;00:05, 14.03it/s] 97%|█████████▋| 1937/2000 [02:00&lt;00:04, 13.95it/s] 97%|█████████▋| 1945/2000 [02:01&lt;00:03, 14.42it/s] 98%|█████████▊| 1953/2000 [02:01&lt;00:02, 16.65it/s] 98%|█████████▊| 1961/2000 [02:02&lt;00:02, 16.19it/s] 98%|█████████▊| 1969/2000 [02:02&lt;00:01, 17.42it/s] 99%|█████████▉| 1977/2000 [02:03&lt;00:01, 15.81it/s] 99%|█████████▉| 1985/2000 [02:03&lt;00:00, 18.45it/s]100%|█████████▉| 1993/2000 [02:03&lt;00:00, 17.57it/s]100%|██████████| 2000/2000 [02:03&lt;00:00, 16.14it/s]\n\n\nFirst 10 Predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\n\n\n\n100%|██████████| 2000/2000 [00:13&lt;00:00, 144.93it/s]\n\nFirst 10 Predictions: [0, 0, 0, 0, 0, 1, 0, 1, 1, 1]\n\nfrom sklearn.metrics import classification_report\n# To evaluate the model's performance, we will create a classification report.\n\n# Extract true labels for the sampled data\ny_true = test_sample[\"label\"]\n\n# Define a function to evaluate performance\ndef evaluate_performance(y_true, y_pred):\n    \"\"\"Create and print the classification report\"\"\"\n    performance = classification_report(\n        y_true, y_pred,\n        target_names=[\"Negative Review\", \"Positive Review\"]\n    )\n    print(performance)\n\n# Evaluate the model performance\nevaluate_performance(y_true, y_pred)\n\n\n\nClassification Tasks That Leverage Embeddings\n\nfrom sentence_transformers import SentenceTransformer\n\n# Load model\nembedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=\"cpu\")\n\n# Convert text to embeddings\ntrain_embeddings = embedding_model.encode(train_sample[\"content\"], show_progress_bar=True)\ntest_embeddings = embedding_model.encode(test_sample[\"content\"], show_progress_bar=True)\n\ntrain_embeddings.shape\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Train a Logistic Regression on our train embeddings\nclf = LogisticRegression(random_state=42)\nclf.fit(train_embeddings, train_sample[\"label\"])\n\n# Predict previously unseen instances\ny_pred_embeddings = clf.predict(test_embeddings)\n\n# Evaluate the performance of the embedding-based classification\nevaluate_performance(test_sample[\"label\"], y_pred_embeddings)\n\n\n\nWhat if we don’t use a classifier at all?\nInstead, we can average the embeddings per class and apply cosine similarity to predict which classes match the documents best:\n\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Average the embeddings of all documents in each target label\ndf = pd.DataFrame(np.hstack([train_embeddings, np.array(train_sample[\"label\"]).reshape(-1, 1)]))\naveraged_target_embeddings = df.groupby(768).mean().values\n\n# Find the best matching embeddings between evaluation documents and target embeddings\nsim_matrix = cosine_similarity(test_embeddings, averaged_target_embeddings)\ny_pred_no_classifier = np.argmax(sim_matrix, axis=1)\n\n# Evaluate the model\nevaluate_performance(test_sample[\"label\"], y_pred_no_classifier)\n\n\n\nReferences"
  },
  {
    "objectID": "posts/first-post/quarto_test.html",
    "href": "posts/first-post/quarto_test.html",
    "title": "Sample Document",
    "section": "",
    "text": "Introduction\nThe difference between the two is shown in Figure 1, adapted from (Alammar and Grootendorst 2024).\n\n\n\n\n\n\nFigure 1: Both representation and generative models can be used for classification, but they take different approaches\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nAlammar, Jay, and Maarten Grootendorst. 2024. Hands-on Large Language Models. O’Reilly Media, Inc."
  },
  {
    "objectID": "posts/first-post/index.html#text-classification-with-representation-models",
    "href": "posts/first-post/index.html#text-classification-with-representation-models",
    "title": "Text Classification with LLMs (part 1)",
    "section": "",
    "text": "When used for classification tasks, representation models can be broadly divided into:\n\nTask-Specific Models: These models involve starting with a general-purpose pre-trained model (such as BERT) and fine-tuning it directly for a specific task, such as sentiment classification. They take the input text and output the class label directly, making it a one-step process that is optimised for classification.\nEmbedding Models: These models are fine-tuned to generate embeddings that capture the meaning of the text. The embeddings are then used as input features for a separate, traditional classifier (e.g., logistic regression) to predict the class label. So this can be thought of as a two-step process: first, encode the text, and secondly classify it with a simpler model.\n\nFigure 2 illustrates these two approaches.\n\n\n\n\n\n\nFigure 2: Classification can be done directly with a task-specific model or indirectly with an embedding model"
  },
  {
    "objectID": "posts/first-post/index.html#selecting-the-model",
    "href": "posts/first-post/index.html#selecting-the-model",
    "title": "Text Classification with LLMs (part 1)",
    "section": "Selecting the model",
    "text": "Selecting the model\nSo which LLM should we use? As of now, there are over 60,000 models available on Hugging Face for text classification and 8,000 for embeddings. As ever with LLMs, picking the right one involves a trade-off between size and performance.\nAs mentioned earlier, BERT is a popular architecture for creating both task-specific and embedding models. Many variations of it are now available. In the book they used one calledRoBERTa, but here we will use a variation of DistilBERT called DistilBERT base model (uncased), a lighter and faster version of BERT that is fine-tuned specifically for sentiment analysis. So it should give good results!"
  },
  {
    "objectID": "posts/first-post/index.html#loading-the-model",
    "href": "posts/first-post/index.html#loading-the-model",
    "title": "Text Classification with LLMs (part 1)",
    "section": "Loading the model",
    "text": "Loading the model\nNow that we’ve picked our task-specific representation model, we can proceed by loading the model:\n\n# Import the pipeline function from the transformers library\nfrom transformers import pipeline\n\n# Path to HF model\nmodel_path = \"distilbert-base-uncased-finetuned-sst-2-english\"\n\n# Load the model into a pipeline\npipe = pipeline(\n    model=model_path,\n    tokenizer=model_path,\n    # return_all_scores=True,\n    top_k=None,\n    device=\"cuda:0\" # change this to \"-1\" if you only have access to a CPU not GPU\n)"
  },
  {
    "objectID": "posts/first-post/index.html#testing-the-model-out",
    "href": "posts/first-post/index.html#testing-the-model-out",
    "title": "Text Classification with LLMs (part 1)",
    "section": "Testing the model out",
    "text": "Testing the model out\nLet’s use our pre-trained BERT-based model to classify some sample reviews from our dataset. We’ll run it on a couple to see how well it predicts the sentiment:\n\nsample_review = train_sample[8][\"content\"]\n\nresult = pipe(sample_review)\nprint(f\"Review: {sample_review}\")\nprint(f\"Sentiment Analysis Result: {result}\")\n\n\nBlah blah"
  }
]